{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61473d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_sum_pool, global_max_pool, GNNExplainer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, ParameterSampler\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================\n",
    "# 1. Setup and Configuration\n",
    "# ============================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': [(64,), (128,), (64, 64), (128, 128), (64, 128, 64)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_epochs': [50, 100, 200],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'dropout_rate': [0.0, 0.2, 0.5],\n",
    "    'weight_decay': [0.0, 0.0001, 0.001, 0.01],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'aggregation_type': ['mean', 'sum', 'max'],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'learning_rate_scheduler': ['constant', 'step', 'cosine']\n",
    "}\n",
    "\n",
    "# Define the number of random samples from the search space\n",
    "n_iter = 100  # Adjust based on computational resources\n",
    "\n",
    "# Generate random hyperparameter combinations\n",
    "param_list = list(ParameterSampler(search_space, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============================\n",
    "# 2. Data Loading and Processing\n",
    "# ============================\n",
    "\n",
    "# Read the main dataset file\n",
    "data_path = 'path_to_your_data.csv'  # Replace with your actual file path\n",
    "data_df = pd.read_csv(data_path).dropna()\n",
    "\n",
    "# Convert DataFrame to tensor\n",
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32)\n",
    "\n",
    "# Define feature names and mapping (ensure these match your CSV columns)\n",
    "features = ['ORP', 'V', 'DO', 'pH', 'SF', 'Spro', 'Sac', 'Sh', 'SSO4', 'SH2S', 'XS', 'SCH4']\n",
    "node_mapping = {feat: idx for idx, feat in enumerate(features)}\n",
    "\n",
    "# Define causal relationships (edges)\n",
    "relationships = [\n",
    "    ('ORP', 'SF'), ('ORP', 'Spro'), ('ORP', 'Sac'), ('ORP', 'Sh'),\n",
    "    ('V', 'Sac'), ('V', 'Sh'), ('V', 'SSO4'), ('V', 'SH2S'), ('V', 'XS'), ('V', 'SCH4'),\n",
    "    ('DO', 'SF'), ('DO', 'Spro'), ('DO', 'Sac'), ('DO', 'Sh'), ('DO', 'SH2S'), ('DO', 'XS'), ('DO', 'SCH4'),\n",
    "    ('pH', 'SF'), ('pH', 'Spro'), ('pH', 'Sac'), ('pH', 'Sh'), ('pH', 'SSO4'), ('pH', 'XS'),\n",
    "    ('SF', 'Spro'), ('SF', 'Sac'), ('Sac', 'SH2S'), ('SSO4', 'SH2S'), ('Sh', 'SH2S'),\n",
    "    ('XS', 'SF'), ('SH2S', 'SCH4'), ('Sac', 'SCH4'), ('Sh', 'SCH4'), ('SF', 'Sh')\n",
    "]\n",
    "\n",
    "# Generate edge_index tensor\n",
    "edge_index = torch.tensor([[node_mapping[src], node_mapping[dst]] for src, dst in relationships], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# ============================\n",
    "# 3. Model Definition\n",
    "# ============================\n",
    "\n",
    "class FlexibleGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate, activation, num_layers, aggregation_type):\n",
    "        super(FlexibleGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        # Aggregation function\n",
    "        if aggregation_type == 'mean':\n",
    "            self.agg_fn = global_mean_pool\n",
    "        elif aggregation_type == 'sum':\n",
    "            self.agg_fn = global_sum_pool\n",
    "        elif aggregation_type == 'max':\n",
    "            self.agg_fn = global_max_pool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation type: {aggregation_type}\")\n",
    "\n",
    "        # Define GAT layers\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            out_dim = hidden_layers[i] if i < len(hidden_layers) else hidden_layers[-1]\n",
    "            heads = 8  # You can make this a hyperparameter if desired\n",
    "            concat = True if i < num_layers - 1 else False  # Don't concatenate in the last layer\n",
    "            self.gat_layers.append(GATConv(prev_dim, out_dim, heads=heads, concat=concat, dropout=dropout_rate))\n",
    "            prev_dim = out_dim * heads if concat else out_dim\n",
    "\n",
    "        # Define a fully connected layer for regression\n",
    "        self.fc = torch.nn.Linear(prev_dim, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = self.activation_fn(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.agg_fn(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================\n",
    "# 4. Dataset Preparation\n",
    "# ============================\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_tensor, edge_index, target_col=-1):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        self.x = data_tensor[:, :-1]\n",
    "        self.y = data_tensor[:, target_col].unsqueeze(1)\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Single graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = Data(x=self.x, edge_index=self.edge_index, y=self.y)\n",
    "        return data\n",
    "\n",
    "dataset = GraphDataset(data_tensor, edge_index)\n",
    "\n",
    "# ============================\n",
    "# 5. Evaluation Function\n",
    "# ============================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            targets.append(data.y.cpu().numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    targets = np.vstack(targets)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, preds)\n",
    "    return {'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}\n",
    "\n",
    "# ============================\n",
    "# 6. Cross-Validation and Hyperparameter Search\n",
    "# ============================\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_result = None\n",
    "best_val_r2 = -np.inf\n",
    "\n",
    "# Initialize a list to store all results\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting randomized search with five-fold cross-validation...\\n\")\n",
    "for idx, params in enumerate(tqdm(param_list, desc=\"Hyperparameter combinations\")):\n",
    "    fold_metrics = {\n",
    "        'train_mae': [],\n",
    "        'train_mse': [],\n",
    "        'train_rmse': [],\n",
    "        'train_r2': [],\n",
    "        'val_mae': [],\n",
    "        'val_mse': [],\n",
    "        'val_rmse': [],\n",
    "        'val_r2': []\n",
    "    }\n",
    "    \n",
    "    # Convert the entire dataset to a NumPy array for indexing\n",
    "    X = data_tensor[:, :-1].numpy()\n",
    "    y = data_tensor[:, -1].numpy()\n",
    "    \n",
    "    # Perform K-Fold cross-validation\n",
    "    for fold, (train_idx_cv, val_idx_cv) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        train_x = torch.tensor(X[train_idx_cv], dtype=torch.float32)\n",
    "        train_y = torch.tensor(y[train_idx_cv], dtype=torch.float32).unsqueeze(1)\n",
    "        val_x = torch.tensor(X[val_idx_cv], dtype=torch.float32)\n",
    "        val_y = torch.tensor(y[val_idx_cv], dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Create masks\n",
    "        num_nodes = data_tensor.size(0)\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        train_mask[train_idx_cv] = True\n",
    "        val_mask[val_idx_cv] = True\n",
    "        \n",
    "        # Create a single Data object with masks\n",
    "        data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1))\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Create DataLoader for the entire graph\n",
    "        loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = FlexibleGNN(\n",
    "            input_dim=X.shape[1],\n",
    "            hidden_layers=params['hidden_layer_sizes'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            activation=params['activation'],\n",
    "            num_layers=params['num_layers'],\n",
    "            aggregation_type=params['aggregation_type']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'rmsprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {params['optimizer']}\")\n",
    "        \n",
    "        # Define learning rate scheduler\n",
    "        if params['learning_rate_scheduler'] == 'constant':\n",
    "            scheduler = None\n",
    "        elif params['learning_rate_scheduler'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "        elif params['learning_rate_scheduler'] == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['num_epochs'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported learning rate scheduler: {params['learning_rate_scheduler']}\")\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Evaluate on training fold\n",
    "        train_preds = out[data.train_mask].detach().cpu().numpy()\n",
    "        train_targets = data.y[data.train_mask].detach().cpu().numpy()\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "        train_mse = mean_squared_error(train_targets, train_preds)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        fold_metrics['train_mae'].append(train_mae)\n",
    "        fold_metrics['train_mse'].append(train_mse)\n",
    "        fold_metrics['train_rmse'].append(train_rmse)\n",
    "        fold_metrics['train_r2'].append(train_r2)\n",
    "        \n",
    "        # Evaluate on validation fold\n",
    "        val_preds = out[data.val_mask].detach().cpu().numpy()\n",
    "        val_targets = data.y[data.val_mask].detach().cpu().numpy()\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_mse = mean_squared_error(val_targets, val_preds)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        fold_metrics['val_mae'].append(val_mae)\n",
    "        fold_metrics['val_mse'].append(val_mse)\n",
    "        fold_metrics['val_rmse'].append(val_rmse)\n",
    "        fold_metrics['val_r2'].append(val_r2)\n",
    "    \n",
    "    # Aggregate metrics across folds\n",
    "    avg_metrics = {metric: np.mean(values) for metric, values in fold_metrics.items()}\n",
    "    avg_metrics['params'] = params\n",
    "    all_results.append(avg_metrics)\n",
    "    \n",
    "    # Update best result based on validation R2\n",
    "    if avg_metrics['val_r2'] > best_val_r2:\n",
    "        best_val_r2 = avg_metrics['val_r2']\n",
    "        best_result = avg_metrics\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis\n",
    "# ============================\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display top 10 hyperparameter combinations based on validation R²\n",
    "top_results = results_df.sort_values(by='val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop hyperparameter combinations based on average validation R²:\")\n",
    "print(top_results[['params', 'val_r2']])\n",
    "\n",
    "# Save all results to Excel for further analysis\n",
    "results_df.to_excel('EcoGNN_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results saved to 'EcoGNN_Model_AllResults.xlsx'\")\n",
    "\n",
    "# Extract best hyperparameters\n",
    "best_params = best_result['params']\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "\n",
    "# ============================\n",
    "# 8. Final Model Training (Optional)\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_model = FlexibleGNN(\n",
    "    input_dim=data_tensor.size(1) - 1,  # Number of features\n",
    "    hidden_layers=best_params['hidden_layer_sizes'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    activation=best_params['activation'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    aggregation_type=best_params['aggregation_type']\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "if best_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {best_params['optimizer']}\")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "if best_params['learning_rate_scheduler'] == 'constant':\n",
    "    scheduler = None\n",
    "elif best_params['learning_rate_scheduler'] == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "elif best_params['learning_rate_scheduler'] == 'cosine':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=best_params['num_epochs'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported learning rate scheduler: {best_params['learning_rate_scheduler']}\")\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Create a single Data object for the entire dataset\n",
    "full_data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1)).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "final_loader = DataLoader([full_data], batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop for the final model\n",
    "print(\"\\nTraining the final model with the best hyperparameters...\\n\")\n",
    "final_model.train()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "    loss = criterion(out, full_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{best_params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Final Evaluation\n",
    "# ============================\n",
    "\n",
    "# Evaluate the final model on the entire dataset\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))\n",
    "    preds = out.cpu().numpy()\n",
    "    targets = full_data.y.cpu().numpy()\n",
    "    final_mae = mean_absolute_error(targets, preds)\n",
    "    final_mse = mean_squared_error(targets, preds)\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    final_r2 = r2_score(targets, preds)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}, MSE: {final_mse:.4f}, RMSE: {final_rmse:.4f}, R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save Best Hyperparameters and Results\n",
    "# ============================\n",
    "\n",
    "# Save the best hyperparameters and performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save to Excel\n",
    "results_summary.to_excel('EcoGNN_Model_BestResults.xlsx', index=False)\n",
    "print(\"\\nBest model results saved to 'EcoGNN_Model_BestResults.xlsx'\")\n",
    "\n",
    "# ============================\n",
    "# 11. Explainability Analysis with Additional Dataset\n",
    "# ============================\n",
    "\n",
    "# Function to load and preprocess additional data\n",
    "def load_additional_data(additional_data_path, features, target_col, scaler=None):\n",
    "    # Read the additional dataset\n",
    "    additional_df = pd.read_csv(additional_data_path).dropna()\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in additional_df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in the additional CSV.\")\n",
    "    \n",
    "    # Ensure that the remaining columns match the features list\n",
    "    if not all(feature in additional_df.columns for feature in features):\n",
    "        missing = set(features) - set(additional_df.columns)\n",
    "        raise ValueError(f\"The following features are missing from the additional CSV: {missing}\")\n",
    "    \n",
    "    # Rearrange columns to match the features list followed by the target\n",
    "    additional_df = additional_df[features + [target_col]]\n",
    "    \n",
    "    # Perform feature scaling using the scaler fitted on the main dataset\n",
    "    if scaler:\n",
    "        additional_df[features] = scaler.transform(additional_df[features])\n",
    "    else:\n",
    "        # If no scaler is provided, fit a new one\n",
    "        scaler = StandardScaler()\n",
    "        additional_df[features] = scaler.fit_transform(additional_df[features])\n",
    "    \n",
    "    # Convert DataFrame to tensor\n",
    "    additional_data_tensor = torch.tensor(additional_df.values, dtype=torch.float32)\n",
    "    \n",
    "    return additional_data_tensor, scaler\n",
    "\n",
    "# Define the path to the additional dataset\n",
    "additional_data_path = 'path_to_additional_data.csv'  # Replace with your actual additional data file path\n",
    "\n",
    "# Load and preprocess additional dataset\n",
    "# Assuming 'SCH4' is the target column in the additional dataset\n",
    "additional_data_tensor, _ = load_additional_data(additional_data_path, features, 'SCH4', scaler=scaler)\n",
    "\n",
    "# Create a Data object for the additional dataset\n",
    "additional_data = Data(x=additional_data_tensor[:, :-1], \n",
    "                      edge_index=edge_index, \n",
    "                      y=additional_data_tensor[:, -1].unsqueeze(1)).to(device)\n",
    "\n",
    "# Initialize GNNExplainer with the final_model\n",
    "explainer = GNNExplainer(final_model, epochs=200)\n",
    "\n",
    "# Initialize arrays to store importance scores\n",
    "all_edge_importances_additional = []\n",
    "all_feature_importances_additional = []\n",
    "\n",
    "# Number of nodes to explain in the additional dataset\n",
    "num_nodes_additional = additional_data.num_nodes  # Explain all nodes\n",
    "\n",
    "print(\"\\nStarting explainability analysis on the additional dataset using GNNExplainer...\\n\")\n",
    "for node_idx in tqdm(range(num_nodes_additional), desc=\"Explaining nodes in additional dataset\"):\n",
    "    # Explain the node\n",
    "    node_feat_mask, edge_mask = explainer.explain_node(node_idx, additional_data.x, additional_data.edge_index)\n",
    "    \n",
    "    # Convert masks to numpy arrays\n",
    "    edge_mask = edge_mask.detach().cpu().numpy()\n",
    "    node_feat_mask = node_feat_mask.detach().cpu().numpy()\n",
    "    \n",
    "    # Handle NaN values by replacing them with 0\n",
    "    edge_mask = np.nan_to_num(edge_mask)\n",
    "    node_feat_mask = np.nan_to_num(node_feat_mask)\n",
    "    \n",
    "    # Normalize the masks to [0, 1]\n",
    "    if np.max(edge_mask) > 0:\n",
    "        edge_mask_normalized = edge_mask / np.max(edge_mask)\n",
    "    else:\n",
    "        edge_mask_normalized = edge_mask  # All zeros\n",
    "    \n",
    "    if np.max(node_feat_mask) > 0:\n",
    "        node_feat_mask_normalized = node_feat_mask / np.max(node_feat_mask)\n",
    "    else:\n",
    "        node_feat_mask_normalized = node_feat_mask  # All zeros\n",
    "    \n",
    "    # Append the normalized masks to the lists\n",
    "    all_edge_importances_additional.append(edge_mask_normalized)\n",
    "    all_feature_importances_additional.append(node_feat_mask_normalized)\n",
    "\n",
    "# Convert lists to numpy arrays for aggregation\n",
    "edge_importances_array_additional = np.array(all_edge_importances_additional)\n",
    "feature_importances_array_additional = np.array(all_feature_importances_additional)\n",
    "\n",
    "# Aggregate importance scores by averaging across all explained nodes\n",
    "mean_edge_importance_additional = edge_importances_array_additional.mean(axis=0)\n",
    "mean_feature_importance_additional = feature_importances_array_additional.mean(axis=0)\n",
    "\n",
    "# Create DataFrames for easy visualization and saving\n",
    "edge_importance_df_additional = pd.DataFrame({\n",
    "    'Source': [src for src, _ in relationships],\n",
    "    'Target': [dst for _, dst in relationships],\n",
    "    'Importance': mean_edge_importance_additional\n",
    "})\n",
    "\n",
    "feature_importance_df_additional = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': mean_feature_importance_additional\n",
    "})\n",
    "\n",
    "# ============================\n",
    "# 12. Save Explainability Analysis Results\n",
    "# ============================\n",
    "\n",
    "# Save the explainability results to Excel\n",
    "with pd.ExcelWriter('EcoGNN_Explainability.xlsx') as writer:\n",
    "    edge_importance_df_additional.to_excel(writer, sheet_name='Edge_Importances_Additional', index=False)\n",
    "    feature_importance_df_additional.to_excel(writer, sheet_name='Feature_Importances_Additional', index=False)\n",
    "\n",
    "print(\"\\nExplainability analysis results on the additional dataset saved to 'EcoGNN_Explainability.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN2",
   "language": "python",
   "name": "gnn2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
