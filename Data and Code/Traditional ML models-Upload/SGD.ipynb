{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbadc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib  # For saving models\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Read the dataset\n",
    "data_path = 'path_to_your_data.csv'  # Replace with your actual file path\n",
    "data_df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"Missing values detected. Performing imputation...\")\n",
    "    # Simple imputation: Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\nData Preprocessing Completed.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Model and Hyperparameter Search Space\n",
    "# ============================\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'model': ['LinearRegression', 'Ridge', 'Lasso', 'ElasticNet'],\n",
    "    'alpha': [0.1, 1.0, 10.0],  # Applicable to Ridge, Lasso, ElasticNet\n",
    "    'l1_ratio': [0.2, 0.5, 0.8],  # Applicable to ElasticNet\n",
    "}\n",
    "\n",
    "# Generate hyperparameter combinations\n",
    "param_grid = []\n",
    "for model in search_space['model']:\n",
    "    if model == 'LinearRegression':\n",
    "        param_grid.append({'model': [model]})\n",
    "    elif model in ['Ridge', 'Lasso']:\n",
    "        param_grid.append({'model': [model], 'alpha': search_space['alpha']})\n",
    "    elif model == 'ElasticNet':\n",
    "        param_grid.append({'model': [model], 'alpha': search_space['alpha'], 'l1_ratio': search_space['l1_ratio']})\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Model Training\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize list to store results for each hyperparameter combination\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting five-fold cross-validation and hyperparameter search...\\n\")\n",
    "\n",
    "# Iterate over each hyperparameter combination\n",
    "for params in tqdm(param_grid, desc=\"Model Combinations\"):\n",
    "    # Initialize metrics storage for current hyperparameter combination\n",
    "    fold_metrics = {\n",
    "        'train_mae': [],\n",
    "        'train_mse': [],\n",
    "        'train_rmse': [],\n",
    "        'train_r2': [],\n",
    "        'val_mae': [],\n",
    "        'val_mse': [],\n",
    "        'val_rmse': [],\n",
    "        'val_r2': []\n",
    "    }\n",
    "    \n",
    "    # Iterate over each fold\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        # Select and initialize the model based on hyperparameters\n",
    "        if params['model'] == 'LinearRegression':\n",
    "            model = LinearRegression()\n",
    "        elif params['model'] == 'Ridge':\n",
    "            model = Ridge(alpha=params['alpha'])\n",
    "        elif params['model'] == 'Lasso':\n",
    "            model = Lasso(alpha=params['alpha'])\n",
    "        elif params['model'] == 'ElasticNet':\n",
    "            model = ElasticNet(alpha=params['alpha'], l1_ratio=params['l1_ratio'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {params['model']}\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions on training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        \n",
    "        # Predictions on validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(y_val, y_val_pred)\n",
    "        \n",
    "        # Store metrics for the current fold\n",
    "        fold_metrics['train_mae'].append(train_mae)\n",
    "        fold_metrics['train_mse'].append(train_mse)\n",
    "        fold_metrics['train_rmse'].append(train_rmse)\n",
    "        fold_metrics['train_r2'].append(train_r2)\n",
    "        fold_metrics['val_mae'].append(val_mae)\n",
    "        fold_metrics['val_mse'].append(val_mse)\n",
    "        fold_metrics['val_rmse'].append(val_rmse)\n",
    "        fold_metrics['val_r2'].append(val_r2)\n",
    "    \n",
    "    # Calculate average metrics across all folds for the current hyperparameter combination\n",
    "    avg_metrics = {metric: np.mean(values) for metric, values in fold_metrics.items()}\n",
    "    avg_metrics['params'] = params\n",
    "    all_results.append(avg_metrics)\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Convert all results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sort the results based on validation R² in descending order and select the top 10\n",
    "top_results = results_df.sort_values(by='val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results[['params', 'val_r2']])\n",
    "\n",
    "# Save all results to an Excel file for further analysis\n",
    "results_df.to_excel('LR_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'LR_Model_AllResults.xlsx'\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Extract the hyperparameter combination with the highest validation R²\n",
    "best_result = results_df.loc[results_df['val_r2'].idxmax()]\n",
    "best_params = best_result['params']\n",
    "print(f\"\\nBest Hyperparameter Combination: {best_params}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize and train the final model using the best hyperparameters on the entire dataset\n",
    "if best_params['model'] == 'LinearRegression':\n",
    "    final_model = LinearRegression()\n",
    "elif best_params['model'] == 'Ridge':\n",
    "    final_model = Ridge(alpha=best_params['alpha'])\n",
    "elif best_params['model'] == 'Lasso':\n",
    "    final_model = Lasso(alpha=best_params['alpha'])\n",
    "elif best_params['model'] == 'ElasticNet':\n",
    "    final_model = ElasticNet(alpha=best_params['alpha'], l1_ratio=best_params['l1_ratio'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model: {best_params['model']}\")\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_model.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model\n",
    "# ============================\n",
    "\n",
    "# Save the scaler and the final trained model using joblib\n",
    "joblib.dump(scaler, 'LR_Scaler.pkl')\n",
    "joblib.dump(final_model, 'LR_FinalModel.pkl')\n",
    "print(\"\\nScaler and final model have been saved.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Save the best hyperparameters and final performance metrics to an Excel file\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save to Excel\n",
    "results_summary.to_excel('LR_Model_BestResults.xlsx', index=False)\n",
    "print(\"\\nBest model results have been saved to 'LR_Model_BestResults.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d270fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  # For saving models\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed for Reproducibility\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for NumPy and Python's random module to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can choose any seed value you prefer\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Define the path to your CSV data file\n",
    "data_path = 'path_to_your_data.csv'  # <-- Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "# Extract features and target variable as NumPy arrays\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values in features and target\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"\\nMissing values detected. Performing imputation...\")\n",
    "    # Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "    print(\"Missing values imputed with column means.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Pipeline and Hyperparameter Grid\n",
    "# ============================\n",
    "\n",
    "# Create a pipeline that first scales the data and then applies SGDRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('sgdregressor', SGDRegressor(\n",
    "        max_iter=1000, \n",
    "        tol=1e-3, \n",
    "        learning_rate='constant',  # To use eta0 effectively\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'sgdregressor__eta0': [0.01, 0.05, 0.1, 0.5],          # Learning rates\n",
    "    'sgdregressor__momentum': [0.1, 0.3, 0.5, 0.7, 0.9]  # Momentum values\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Grid Search\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and hyperparameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',          # Using R² as the evaluation metric\n",
    "    cv=kf,\n",
    "    n_jobs=-1,             # Utilize all available CPU cores\n",
    "    verbose=2              # Verbosity level: 0, 1, or 2\n",
    ")\n",
    "\n",
    "print(\"Starting five-fold cross-validation and grid search...\\n\")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Extract all grid search results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for clarity\n",
    "selected_columns = [\n",
    "    'param_sgdregressor__eta0',\n",
    "    'param_sgdregressor__momentum',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'std_train_score'\n",
    "]\n",
    "results_selected = results_df[selected_columns]\n",
    "\n",
    "# Rename columns for better readability\n",
    "results_selected = results_selected.rename(columns={\n",
    "    'param_sgdregressor__eta0': 'eta0',\n",
    "    'param_sgdregressor__momentum': 'momentum',\n",
    "    'mean_test_score': 'mean_val_r2',\n",
    "    'std_test_score': 'std_val_r2',\n",
    "    'mean_train_score': 'mean_train_r2',\n",
    "    'std_train_score': 'std_train_r2'\n",
    "})\n",
    "\n",
    "# Sort the results by mean validation R² in descending order and select top 10\n",
    "top_results = results_selected.sort_values(by='mean_val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results)\n",
    "\n",
    "# Save all grid search results to an Excel file for further analysis\n",
    "results_selected.to_excel('SGD_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'SGD_Model_AllResults.xlsx'.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best validation R² score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Validation R² Score: {best_score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model and Scaler\n",
    "# ============================\n",
    "\n",
    "# Save the pipeline (which includes both scaler and model) using joblib\n",
    "model_filename = 'SGD_FinalModel_Pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"\\nFinal model pipeline (including scaler) has been saved as '{model_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Create a dictionary with the best hyperparameters and final performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_filename = 'SGD_Model_BestResults.xlsx'\n",
    "results_summary.to_excel(summary_filename, index=False)\n",
    "print(f\"\\nBest model results have been saved to '{summary_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 12. Conclusion\n",
    "# ============================\n",
    "\n",
    "print(\"\\nScript execution completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34994c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  # For saving models\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed for Reproducibility\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for NumPy and Python's random module to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can choose any seed value you prefer\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Define the path to your CSV data file\n",
    "data_path = 'path_to_your_data.csv'  # <-- Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "# Extract features and target variable as NumPy arrays\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values in features and target\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"\\nMissing values detected. Performing imputation...\")\n",
    "    # Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "    print(\"Missing values imputed with column means.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Pipeline and Hyperparameter Grid\n",
    "# ============================\n",
    "\n",
    "# Create a pipeline that first scales the data and then applies RandomForestRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('randomforestregressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'randomforestregressor__n_estimators': [100, 200, 300, 500],       # Number of trees\n",
    "    'randomforestregressor__max_depth': [None, 10, 20, 30],           # Maximum depth of the tree\n",
    "    'randomforestregressor__min_samples_split': [2, 5, 10],           # Minimum number of samples required to split an internal node\n",
    "    'randomforestregressor__min_samples_leaf': [1, 2, 4],             # Minimum number of samples required to be at a leaf node\n",
    "    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2']    # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Grid Search\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and hyperparameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',          # Using R² as the evaluation metric\n",
    "    cv=kf,\n",
    "    n_jobs=-1,             # Utilize all available CPU cores\n",
    "    verbose=2              # Verbosity level: 0, 1, or 2\n",
    ")\n",
    "\n",
    "print(\"Starting five-fold cross-validation and grid search...\\n\")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Extract all grid search results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for clarity\n",
    "selected_columns = [\n",
    "    'param_randomforestregressor__n_estimators',\n",
    "    'param_randomforestregressor__max_depth',\n",
    "    'param_randomforestregressor__min_samples_split',\n",
    "    'param_randomforestregressor__min_samples_leaf',\n",
    "    'param_randomforestregressor__max_features',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'std_train_score'\n",
    "]\n",
    "results_selected = results_df[selected_columns]\n",
    "\n",
    "# Rename columns for better readability\n",
    "results_selected = results_selected.rename(columns={\n",
    "    'param_randomforestregressor__n_estimators': 'n_estimators',\n",
    "    'param_randomforestregressor__max_depth': 'max_depth',\n",
    "    'param_randomforestregressor__min_samples_split': 'min_samples_split',\n",
    "    'param_randomforestregressor__min_samples_leaf': 'min_samples_leaf',\n",
    "    'param_randomforestregressor__max_features': 'max_features',\n",
    "    'mean_test_score': 'mean_val_r2',\n",
    "    'std_test_score': 'std_val_r2',\n",
    "    'mean_train_score': 'mean_train_r2',\n",
    "    'std_train_score': 'std_train_r2'\n",
    "})\n",
    "\n",
    "# Sort the results by mean validation R² in descending order and select top 10\n",
    "top_results = results_selected.sort_values(by='mean_val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results)\n",
    "\n",
    "# Save all grid search results to an Excel file for further analysis\n",
    "results_selected.to_excel('RF_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'RF_Model_AllResults.xlsx'.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best validation R² score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Validation R² Score: {best_score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model and Scaler\n",
    "# ============================\n",
    "\n",
    "# Save the pipeline (which includes both scaler and model) using joblib\n",
    "model_filename = 'RF_FinalModel_Pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"\\nFinal model pipeline (including scaler) has been saved as '{model_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Create a dictionary with the best hyperparameters and final performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_filename = 'RF_Model_BestResults.xlsx'\n",
    "results_summary.to_excel(summary_filename, index=False)\n",
    "print(f\"\\nBest model results have been saved to '{summary_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 12. Conclusion\n",
    "# ============================\n",
    "\n",
    "print(\"\\nScript execution completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1030a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  # For saving models\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Import LightGBM's LGBMRegressor\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except ImportError:\n",
    "    print(\"LightGBM is not installed. Installing now...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"lightgbm\"])\n",
    "    from lightgbm import LGBMRegressor\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed for Reproducibility\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for NumPy and Python's random module to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can choose any seed value you prefer\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Define the path to your CSV data file\n",
    "data_path = 'path_to_your_data.csv'  # <-- Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "# Extract features and target variable as NumPy arrays\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values in features and target\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"\\nMissing values detected. Performing imputation...\")\n",
    "    # Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "    print(\"Missing values imputed with column means.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Pipeline and Hyperparameter Grid\n",
    "# ============================\n",
    "\n",
    "# Create a pipeline that first scales the data and then applies LGBMRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lgbmregressor', LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'lgbmregressor__num_leaves': [31, 64, 128, 256],          # Number of leaves in full trees\n",
    "    'lgbmregressor__max_depth': [10, 20, 30, 40],            # Maximum depth of the tree\n",
    "    'lgbmregressor__n_estimators': [100, 200, 300],          # Number of boosting iterations\n",
    "    'lgbmregressor__learning_rate': [0.01, 0.1, 0.5],        # Boosting learning rate\n",
    "    'lgbmregressor__subsample': [0.6, 0.8, 1.0],             # Fraction of samples to be used for fitting the individual base learners\n",
    "    'lgbmregressor__colsample_bytree': ['auto', 'sqrt', 'log2'],  # Fraction of features to be used for fitting the individual base learners\n",
    "    'lgbmregressor__reg_alpha': [0.0, 0.1, 0.5, 1.0],        # L1 regularization term on weights\n",
    "    'lgbmregressor__reg_lambda': [0.0, 0.1, 0.5, 1.0]        # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Grid Search\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and hyperparameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',          # Using R² as the evaluation metric\n",
    "    cv=kf,\n",
    "    n_jobs=-1,             # Utilize all available CPU cores\n",
    "    verbose=2              # Verbosity level: 0, 1, or 2\n",
    ")\n",
    "\n",
    "print(\"Starting five-fold cross-validation and grid search...\\n\")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Extract all grid search results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for clarity\n",
    "selected_columns = [\n",
    "    'param_lgbmregressor__num_leaves',\n",
    "    'param_lgbmregressor__max_depth',\n",
    "    'param_lgbmregressor__n_estimators',\n",
    "    'param_lgbmregressor__learning_rate',\n",
    "    'param_lgbmregressor__subsample',\n",
    "    'param_lgbmregressor__colsample_bytree',\n",
    "    'param_lgbmregressor__reg_alpha',\n",
    "    'param_lgbmregressor__reg_lambda',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'std_train_score'\n",
    "]\n",
    "results_selected = results_df[selected_columns]\n",
    "\n",
    "# Rename columns for better readability\n",
    "results_selected = results_selected.rename(columns={\n",
    "    'param_lgbmregressor__num_leaves': 'num_leaves',\n",
    "    'param_lgbmregressor__max_depth': 'max_depth',\n",
    "    'param_lgbmregressor__n_estimators': 'n_estimators',\n",
    "    'param_lgbmregressor__learning_rate': 'learning_rate',\n",
    "    'param_lgbmregressor__subsample': 'subsample',\n",
    "    'param_lgbmregressor__colsample_bytree': 'colsample_bytree',\n",
    "    'param_lgbmregressor__reg_alpha': 'reg_alpha',\n",
    "    'param_lgbmregressor__reg_lambda': 'reg_lambda',\n",
    "    'mean_test_score': 'mean_val_r2',\n",
    "    'std_test_score': 'std_val_r2',\n",
    "    'mean_train_score': 'mean_train_r2',\n",
    "    'std_train_score': 'std_train_r2'\n",
    "})\n",
    "\n",
    "# Sort the results by mean validation R² in descending order and select top 10\n",
    "top_results = results_selected.sort_values(by='mean_val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results)\n",
    "\n",
    "# Save all grid search results to an Excel file for further analysis\n",
    "results_selected.to_excel('LGBM_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'LGBM_Model_AllResults.xlsx'.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best validation R² score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Validation R² Score: {best_score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model and Scaler\n",
    "# ============================\n",
    "\n",
    "# Save the pipeline (which includes both scaler and model) using joblib\n",
    "model_filename = 'LGBM_FinalModel_Pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"\\nFinal model pipeline (including scaler) has been saved as '{model_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Create a dictionary with the best hyperparameters and final performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_filename = 'LGBM_Model_BestResults.xlsx'\n",
    "results_summary.to_excel(summary_filename, index=False)\n",
    "print(f\"\\nBest model results have been saved to '{summary_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 12. Conclusion\n",
    "# ============================\n",
    "\n",
    "print(\"\\nScript execution completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  # For saving models\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed for Reproducibility\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for NumPy and Python's random module to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can choose any seed value you prefer\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Define the path to your CSV data file\n",
    "data_path = 'path_to_your_data.csv'  # <-- Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "# Extract features and target variable as NumPy arrays\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values in features and target\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"\\nMissing values detected. Performing imputation...\")\n",
    "    # Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "    print(\"Missing values imputed with column means.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Pipeline and Hyperparameter Grid\n",
    "# ============================\n",
    "\n",
    "# Create a pipeline that first scales the data and then applies MLPRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlpregressor', MLPRegressor(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "# Since some hyperparameters are only applicable to certain solvers,\n",
    "# we define separate parameter grids for each solver\n",
    "param_grid = [\n",
    "    {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs'],\n",
    "        'mlpregressor__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'mlpregressor__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'mlpregressor__max_iter': [200, 300, 500],\n",
    "        'mlpregressor__batch_size': [32, 64, 128, 256],\n",
    "        'mlpregressor__early_stopping': [True, False],\n",
    "        'mlpregressor__validation_fraction': [0.1, 0.2, 0.3]\n",
    "        # Note: 'momentum', 'beta_1', 'beta_2', 'epsilon' are not applicable to 'lbfgs'\n",
    "    },\n",
    "    {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['sgd'],\n",
    "        'mlpregressor__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'mlpregressor__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'mlpregressor__max_iter': [200, 300, 500],\n",
    "        'mlpregressor__batch_size': [32, 64, 128, 256],\n",
    "        'mlpregressor__momentum': [0.0, 0.5, 0.9],\n",
    "        'mlpregressor__early_stopping': [True, False],\n",
    "        'mlpregressor__validation_fraction': [0.1, 0.2, 0.3]\n",
    "        # 'beta_1', 'beta_2', 'epsilon' are not applicable to 'sgd'\n",
    "    },\n",
    "    {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['adam'],\n",
    "        'mlpregressor__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'mlpregressor__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'mlpregressor__max_iter': [200, 300, 500],\n",
    "        'mlpregressor__batch_size': [32, 64, 128, 256],\n",
    "        'mlpregressor__beta_1': [0.9, 0.95, 0.99],\n",
    "        'mlpregressor__beta_2': [0.999, 0.995, 0.99],\n",
    "        'mlpregressor__epsilon': [1e-8, 1e-7, 1e-6],\n",
    "        'mlpregressor__early_stopping': [True, False],\n",
    "        'mlpregressor__validation_fraction': [0.1, 0.2, 0.3]\n",
    "        # 'momentum' is not applicable to 'adam'\n",
    "    }\n",
    "]\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Grid Search\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and hyperparameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',          # Using R² as the evaluation metric\n",
    "    cv=kf,\n",
    "    n_jobs=-1,             # Utilize all available CPU cores\n",
    "    verbose=2              # Verbosity level: 0, 1, or 2\n",
    ")\n",
    "\n",
    "print(\"Starting five-fold cross-validation and grid search...\\n\")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Extract all grid search results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for clarity\n",
    "selected_columns = [\n",
    "    'param_mlpregressor__hidden_layer_sizes',\n",
    "    'param_mlpregressor__activation',\n",
    "    'param_mlpregressor__solver',\n",
    "    'param_mlpregressor__alpha',\n",
    "    'param_mlpregressor__learning_rate',\n",
    "    'param_mlpregressor__learning_rate_init',\n",
    "    'param_mlpregressor__max_iter',\n",
    "    'param_mlpregressor__batch_size',\n",
    "    'param_mlpregressor__momentum',\n",
    "    'param_mlpregressor__beta_1',\n",
    "    'param_mlpregressor__beta_2',\n",
    "    'param_mlpregressor__epsilon',\n",
    "    'param_mlpregressor__early_stopping',\n",
    "    'param_mlpregressor__validation_fraction',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'std_train_score'\n",
    "]\n",
    "results_selected = results_df[selected_columns]\n",
    "\n",
    "# Rename columns for better readability\n",
    "results_selected = results_selected.rename(columns={\n",
    "    'param_mlpregressor__hidden_layer_sizes': 'hidden_layer_sizes',\n",
    "    'param_mlpregressor__activation': 'activation',\n",
    "    'param_mlpregressor__solver': 'solver',\n",
    "    'param_mlpregressor__alpha': 'alpha',\n",
    "    'param_mlpregressor__learning_rate': 'learning_rate',\n",
    "    'param_mlpregressor__learning_rate_init': 'learning_rate_init',\n",
    "    'param_mlpregressor__max_iter': 'max_iter',\n",
    "    'param_mlpregressor__batch_size': 'batch_size',\n",
    "    'param_mlpregressor__momentum': 'momentum',\n",
    "    'param_mlpregressor__beta_1': 'beta_1',\n",
    "    'param_mlpregressor__beta_2': 'beta_2',\n",
    "    'param_mlpregressor__epsilon': 'epsilon',\n",
    "    'param_mlpregressor__early_stopping': 'early_stopping',\n",
    "    'param_mlpregressor__validation_fraction': 'validation_fraction',\n",
    "    'mean_test_score': 'mean_val_r2',\n",
    "    'std_test_score': 'std_val_r2',\n",
    "    'mean_train_score': 'mean_train_r2',\n",
    "    'std_train_score': 'std_train_r2'\n",
    "})\n",
    "\n",
    "# Sort the results by mean validation R² in descending order and select top 10\n",
    "top_results = results_selected.sort_values(by='mean_val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results)\n",
    "\n",
    "# Save all grid search results to an Excel file for further analysis\n",
    "results_selected.to_excel('ANN_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'ANN_Model_AllResults.xlsx'.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best validation R² score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Validation R² Score: {best_score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model and Scaler\n",
    "# ============================\n",
    "\n",
    "# Save the pipeline (which includes both scaler and model) using joblib\n",
    "model_filename = 'ANN_FinalModel_Pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"\\nFinal model pipeline (including scaler) has been saved as '{model_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Create a dictionary with the best hyperparameters and final performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_filename = 'ANN_Model_BestResults.xlsx'\n",
    "results_summary.to_excel(summary_filename, index=False)\n",
    "print(f\"\\nBest model results have been saved to '{summary_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 12. Conclusion\n",
    "# ============================\n",
    "\n",
    "print(\"\\nScript execution completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib  # For saving models\n",
    "from sklearn.neural_network import BernoulliRBM  # RBM implementation\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed for Reproducibility\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for NumPy and Python's random module to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can choose any seed value you prefer\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Define the path to your CSV data file\n",
    "data_path = 'path_to_your_data.csv'  # <-- Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "# Extract features and target variable as NumPy arrays\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values in features and target\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"\\nMissing values detected. Performing imputation...\")\n",
    "    # Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "    print(\"Missing values imputed with column means.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Pipeline and Hyperparameter Grid\n",
    "# ============================\n",
    "\n",
    "# Create a pipeline that first scales the data, applies RBM, and then uses MLPRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rbm', BernoulliRBM(random_state=42)),\n",
    "    ('mlpregressor', MLPRegressor(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "# Note: 'momentum', 'regularization', and 'init_weight' are NOT applicable to BernoulliRBM in scikit-learn\n",
    "param_grid = {\n",
    "    'rbm__n_components': [64, 128, 256, 512],\n",
    "    'rbm__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'rbm__batch_size': [10, 50, 100, 200],\n",
    "    'rbm__n_iter': [10, 50, 100, 200],\n",
    "    'rbm__verbose': [0, 1],\n",
    "    # 'rbm__momentum': [0.0, 0.5, 0.9],  # Not applicable\n",
    "    # 'rbm__regularization': [0.0, 0.0001, 0.001, 0.01],  # Not applicable\n",
    "    # 'rbm__init_weight': [0.01, 0.1, 0.5],  # Not applicable\n",
    "    # You can add more RBM parameters if needed\n",
    "    \n",
    "    # Optionally, you can also tune MLPRegressor hyperparameters\n",
    "    'mlpregressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],\n",
    "    'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'mlpregressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'mlpregressor__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'mlpregressor__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'mlpregressor__max_iter': [200, 300, 500],\n",
    "    'mlpregressor__batch_size': [32, 64, 128, 256],\n",
    "    'mlpregressor__momentum': [0.0, 0.5, 0.9],\n",
    "    'mlpregressor__early_stopping': [True, False],\n",
    "    'mlpregressor__validation_fraction': [0.1, 0.2, 0.3],\n",
    "    'mlpregressor__beta_1': [0.9, 0.95, 0.99],\n",
    "    'mlpregressor__beta_2': [0.999, 0.995, 0.99],\n",
    "    'mlpregressor__epsilon': [1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Grid Search\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and hyperparameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',          # Using R² as the evaluation metric\n",
    "    cv=kf,\n",
    "    n_jobs=-1,             # Utilize all available CPU cores\n",
    "    verbose=2              # Verbosity level: 0, 1, or 2\n",
    ")\n",
    "\n",
    "print(\"Starting five-fold cross-validation and grid search...\\n\")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Extract all grid search results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for clarity\n",
    "selected_columns = [\n",
    "    'param_rbm__n_components',\n",
    "    'param_rbm__learning_rate',\n",
    "    'param_rbm__batch_size',\n",
    "    'param_rbm__n_iter',\n",
    "    'param_rbm__verbose',\n",
    "    'param_mlpregressor__hidden_layer_sizes',\n",
    "    'param_mlpregressor__activation',\n",
    "    'param_mlpregressor__solver',\n",
    "    'param_mlpregressor__alpha',\n",
    "    'param_mlpregressor__learning_rate',\n",
    "    'param_mlpregressor__learning_rate_init',\n",
    "    'param_mlpregressor__max_iter',\n",
    "    'param_mlpregressor__batch_size',\n",
    "    'param_mlpregressor__momentum',\n",
    "    'param_mlpregressor__early_stopping',\n",
    "    'param_mlpregressor__validation_fraction',\n",
    "    'param_mlpregressor__beta_1',\n",
    "    'param_mlpregressor__beta_2',\n",
    "    'param_mlpregressor__epsilon',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'std_train_score'\n",
    "]\n",
    "results_selected = results_df[selected_columns]\n",
    "\n",
    "# Rename columns for better readability\n",
    "results_selected = results_selected.rename(columns={\n",
    "    'param_rbm__n_components': 'n_components',\n",
    "    'param_rbm__learning_rate': 'rbm_learning_rate',\n",
    "    'param_rbm__batch_size': 'rbm_batch_size',\n",
    "    'param_rbm__n_iter': 'rbm_n_iter',\n",
    "    'param_rbm__verbose': 'rbm_verbose',\n",
    "    'param_mlpregressor__hidden_layer_sizes': 'hidden_layer_sizes',\n",
    "    'param_mlpregressor__activation': 'activation',\n",
    "    'param_mlpregressor__solver': 'solver',\n",
    "    'param_mlpregressor__alpha': 'alpha',\n",
    "    'param_mlpregressor__learning_rate': 'mlp_learning_rate',\n",
    "    'param_mlpregressor__learning_rate_init': 'learning_rate_init',\n",
    "    'param_mlpregressor__max_iter': 'mlp_max_iter',\n",
    "    'param_mlpregressor__batch_size': 'mlp_batch_size',\n",
    "    'param_mlpregressor__momentum': 'momentum',\n",
    "    'param_mlpregressor__early_stopping': 'early_stopping',\n",
    "    'param_mlpregressor__validation_fraction': 'validation_fraction',\n",
    "    'param_mlpregressor__beta_1': 'beta_1',\n",
    "    'param_mlpregressor__beta_2': 'beta_2',\n",
    "    'param_mlpregressor__epsilon': 'epsilon',\n",
    "    'mean_test_score': 'mean_val_r2',\n",
    "    'std_test_score': 'std_val_r2',\n",
    "    'mean_train_score': 'mean_train_r2',\n",
    "    'std_train_score': 'std_train_r2'\n",
    "})\n",
    "\n",
    "# Sort the results by mean validation R² in descending order and select top 10\n",
    "top_results = results_selected.sort_values(by='mean_val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results)\n",
    "\n",
    "# Save all grid search results to an Excel file for further analysis\n",
    "results_selected.to_excel('ANN_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'ANN_Model_AllResults.xlsx'.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best validation R² score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Validation R² Score: {best_score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model and Scaler\n",
    "# ============================\n",
    "\n",
    "# Save the pipeline (which includes both scaler, RBM, and MLPRegressor) using joblib\n",
    "model_filename = 'ANN_FinalModel_Pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"\\nFinal model pipeline (including scaler and RBM) has been saved as '{model_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Create a dictionary with the best hyperparameters and final performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_filename = 'ANN_Model_BestResults.xlsx'\n",
    "results_summary.to_excel(summary_filename, index=False)\n",
    "print(f\"\\nBest model results have been saved to '{summary_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 12. Conclusion\n",
    "# ============================\n",
    "\n",
    "print(\"\\nScript execution completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ac886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_sum_pool, global_max_pool\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, ParameterSampler\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "from itertools import product  # 引入 itertools\n",
    "\n",
    "# ============================\n",
    "# 1. Setup and Configuration\n",
    "# ============================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': [(64,), (128,), (64, 64), (128, 128), (64, 128, 64)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_epochs': [50, 100, 200],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'dropout_rate': [0.0, 0.2, 0.5],\n",
    "    'weight_decay': [0.0, 0.0001, 0.001, 0.01],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'aggregation_type': ['mean', 'sum', 'max'],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'learning_rate_scheduler': ['constant', 'step', 'cosine']\n",
    "}\n",
    "\n",
    "# Define the number of random samples from the search space\n",
    "n_iter = 100  # Adjust based on computational resources\n",
    "\n",
    "# Generate random hyperparameter combinations\n",
    "param_list = list(ParameterSampler(search_space, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============================\n",
    "# 2. Data Loading and Processing\n",
    "# ============================\n",
    "\n",
    "# Read the single dataset file\n",
    "data_path = 'path_to_your_data.csv'  # Replace with your actual file path\n",
    "data_df = pd.read_csv(data_path).dropna()\n",
    "\n",
    "# Convert DataFrame to tensor\n",
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32)\n",
    "\n",
    "# Define feature names and mapping (ensure these match your CSV columns)\n",
    "features = ['ORP', 'V', 'DO', 'pH', 'SF', 'Spro', 'Sac', 'Sh', 'SSO4', 'SH2S', 'XS', 'SCH4']\n",
    "node_mapping = {feat: idx for idx, feat in enumerate(features)}\n",
    "\n",
    "# ============================\n",
    "# 3. Generate Fully Connected Directed Edge Index\n",
    "# ============================\n",
    "\n",
    "# Generate all possible edges excluding self-connections for a fully connected directed graph\n",
    "num_nodes = len(features)\n",
    "edges = [(i, j) for i, j in product(range(num_nodes), repeat=2) if i != j]\n",
    "\n",
    "# Convert to tensor format and transpose to [2, num_edges]\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# ============================\n",
    "# 4. Model Definition\n",
    "# ============================\n",
    "\n",
    "class FlexibleGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate, activation, num_layers, aggregation_type):\n",
    "        super(FlexibleGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        # Aggregation function\n",
    "        if aggregation_type == 'mean':\n",
    "            self.agg_fn = global_mean_pool\n",
    "        elif aggregation_type == 'sum':\n",
    "            self.agg_fn = global_sum_pool\n",
    "        elif aggregation_type == 'max':\n",
    "            self.agg_fn = global_max_pool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation type: {aggregation_type}\")\n",
    "\n",
    "        # Define GAT layers\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            out_dim = hidden_layers[i] if i < len(hidden_layers) else hidden_layers[-1]\n",
    "            heads = 8  # You can make this a hyperparameter if desired\n",
    "            concat = True if i < num_layers - 1 else False  # Don't concatenate in the last layer\n",
    "            self.gat_layers.append(GATConv(prev_dim, out_dim, heads=heads, concat=concat, dropout=dropout_rate))\n",
    "            prev_dim = out_dim * heads if concat else out_dim\n",
    "\n",
    "        # Define a fully connected layer for regression\n",
    "        self.fc = torch.nn.Linear(prev_dim, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = self.activation_fn(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.agg_fn(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================\n",
    "# 5. Dataset Preparation\n",
    "# ============================\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_tensor, edge_index, target_col=-1):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        self.x = data_tensor[:, :-1]\n",
    "        self.y = data_tensor[:, target_col].unsqueeze(1)\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Single graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = Data(x=self.x, edge_index=self.edge_index, y=self.y)\n",
    "        return data\n",
    "\n",
    "dataset = GraphDataset(data_tensor, edge_index)\n",
    "\n",
    "# ============================\n",
    "# 6. Evaluation Function\n",
    "# ============================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            targets.append(data.y.cpu().numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    targets = np.vstack(targets)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, preds)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# ============================\n",
    "# 7. Cross-Validation and Hyperparameter Search\n",
    "# ============================\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_result = None\n",
    "best_val_r2 = -np.inf\n",
    "\n",
    "# Initialize a list to store all results\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting randomized search with five-fold cross-validation...\\n\")\n",
    "for idx, params in enumerate(tqdm(param_list, desc=\"Hyperparameter combinations\")):\n",
    "    fold_metrics = {\n",
    "        'train_mae': [],\n",
    "        'train_mse': [],\n",
    "        'train_rmse': [],\n",
    "        'train_r2': [],\n",
    "        'val_mae': [],\n",
    "        'val_mse': [],\n",
    "        'val_rmse': [],\n",
    "        'val_r2': []\n",
    "    }\n",
    "    \n",
    "    # Convert the entire dataset to a NumPy array for indexing\n",
    "    X = data_tensor[:, :-1].numpy()\n",
    "    y = data_tensor[:, -1].numpy()\n",
    "    \n",
    "    # Perform K-Fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        train_x = torch.tensor(X[train_idx], dtype=torch.float32)\n",
    "        train_y = torch.tensor(y[train_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        val_x = torch.tensor(X[val_idx], dtype=torch.float32)\n",
    "        val_y = torch.tensor(y[val_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Create masks\n",
    "        num_nodes = data_tensor.size(0)\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        \n",
    "        # Create a single Data object with masks\n",
    "        data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1))\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Create DataLoader for the entire graph\n",
    "        loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = FlexibleGNN(\n",
    "            input_dim=X.shape[1],\n",
    "            hidden_layers=params['hidden_layer_sizes'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            activation=params['activation'],\n",
    "            num_layers=params['num_layers'],\n",
    "            aggregation_type=params['aggregation_type']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'rmsprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {params['optimizer']}\")\n",
    "        \n",
    "        # Define learning rate scheduler\n",
    "        if params['learning_rate_scheduler'] == 'constant':\n",
    "            scheduler = None\n",
    "        elif params['learning_rate_scheduler'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "        elif params['learning_rate_scheduler'] == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['num_epochs'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported learning rate scheduler: {params['learning_rate_scheduler']}\")\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Evaluate on training fold\n",
    "        train_preds = out[data.train_mask].detach().cpu().numpy()\n",
    "        train_targets = data.y[data.train_mask].detach().cpu().numpy()\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "        train_mse = mean_squared_error(train_targets, train_preds)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        fold_metrics['train_mae'].append(train_mae)\n",
    "        fold_metrics['train_mse'].append(train_mse)\n",
    "        fold_metrics['train_rmse'].append(train_rmse)\n",
    "        fold_metrics['train_r2'].append(train_r2)\n",
    "        \n",
    "        # Evaluate on validation fold\n",
    "        val_preds = out[data.val_mask].detach().cpu().numpy()\n",
    "        val_targets = data.y[data.val_mask].detach().cpu().numpy()\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_mse = mean_squared_error(val_targets, val_preds)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        fold_metrics['val_mae'].append(val_mae)\n",
    "        fold_metrics['val_mse'].append(val_mse)\n",
    "        fold_metrics['val_rmse'].append(val_rmse)\n",
    "        fold_metrics['val_r2'].append(val_r2)\n",
    "    \n",
    "    # Aggregate metrics across folds\n",
    "    avg_metrics = {metric: np.mean(values) for metric, values in fold_metrics.items()}\n",
    "    avg_metrics['params'] = params\n",
    "    all_results.append(avg_metrics)\n",
    "    \n",
    "    # Update best result based on validation R2\n",
    "    if avg_metrics['val_r2'] > best_val_r2:\n",
    "        best_val_r2 = avg_metrics['val_r2']\n",
    "        best_result = avg_metrics\n",
    "\n",
    "# ============================\n",
    "# 8. Results Analysis\n",
    "# ============================\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display top 10 hyperparameter combinations based on validation R²\n",
    "top_results = results_df.sort_values(by='val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop hyperparameter combinations based on average validation R²:\")\n",
    "print(top_results[['params', 'val_r2']])\n",
    "\n",
    "# Save all results to Excel for further analysis\n",
    "results_df.to_excel('EcoGNNfull_Model_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results saved to 'EcoGNNfull_Model_Model_AllResults.xlsx'\")\n",
    "\n",
    "# Extract best hyperparameters\n",
    "best_params = best_result['params']\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Final Model Training (Optional)\n",
    "# ============================\n",
    "\n",
    "# Optionally, retrain the model on the entire dataset using the best hyperparameters\n",
    "# Note: This step is optional and depends on whether you need a final model for deployment\n",
    "\n",
    "# Initialize the final model\n",
    "final_model = FlexibleGNN(\n",
    "    input_dim=data_tensor.size(1) - 1,  # Number of features\n",
    "    hidden_layers=best_params['hidden_layer_sizes'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    activation=best_params['activation'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    aggregation_type=best_params['aggregation_type']\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "if best_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {best_params['optimizer']}\")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "if best_params['learning_rate_scheduler'] == 'constant':\n",
    "    scheduler = None\n",
    "elif best_params['learning_rate_scheduler'] == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "elif best_params['learning_rate_scheduler'] == 'cosine':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=best_params['num_epochs'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported learning rate scheduler: {best_params['learning_rate_scheduler']}\")\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Create a single Data object for the entire dataset\n",
    "full_data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1)).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "final_loader = DataLoader([full_data], batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop for the final model\n",
    "final_model.train()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "    loss = criterion(out, full_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{best_params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Final Evaluation\n",
    "# ============================\n",
    "\n",
    "# Evaluate the final model using cross-validation metrics\n",
    "# Since we've already used cross-validation to select hyperparameters, we'll skip re-evaluating\n",
    "\n",
    "# However, if you retrain the model on the entire dataset, you can compute metrics directly\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))\n",
    "    preds = out.cpu().numpy()\n",
    "    targets = full_data.y.cpu().numpy()\n",
    "    final_mae = mean_absolute_error(targets, preds)\n",
    "    final_mse = mean_squared_error(targets, preds)\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    final_r2 = r2_score(targets, preds)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}, MSE: {final_mse:.4f}, RMSE: {final_rmse:.4f}, R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Results\n",
    "# ============================\n",
    "\n",
    "# Save the best hyperparameters and performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save to Excel\n",
    "results_summary.to_excel('GNNfull_Model_BestResults.xlsx', index=False)\n",
    "print(\"\\nBest model results saved to 'EcoGNNfull_Model_BestResults.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_sum_pool, global_max_pool\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, ParameterSampler\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# 1. Setup and Configuration\n",
    "# ============================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': [(64,), (128,), (64, 64), (128, 128), (64, 128, 64)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_epochs': [50, 100, 200],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'dropout_rate': [0.0, 0.2, 0.5],\n",
    "    'weight_decay': [0.0, 0.0001, 0.001, 0.01],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'aggregation_type': ['mean', 'sum', 'max'],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'learning_rate_scheduler': ['constant', 'step', 'cosine']\n",
    "}\n",
    "\n",
    "# Define the number of random samples from the search space\n",
    "n_iter = 100  # Adjust based on computational resources\n",
    "\n",
    "# Generate random hyperparameter combinations\n",
    "param_list = list(ParameterSampler(search_space, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============================\n",
    "# 2. Data Loading and Processing\n",
    "# ============================\n",
    "\n",
    "# Read the single dataset file\n",
    "data_path = 'path_to_your_data.csv'  # Replace with your actual file path\n",
    "data_df = pd.read_csv(data_path).dropna()\n",
    "\n",
    "# Convert DataFrame to tensor\n",
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32)\n",
    "\n",
    "# Define feature names and mapping (ensure these match your CSV columns)\n",
    "features = ['ORP', 'V', 'DO', 'pH', 'SF', 'Spro', 'Sac', 'Sh', 'SSO4', 'SH2S', 'XS', 'SCH4']\n",
    "node_mapping = {feat: idx for idx, feat in enumerate(features)}\n",
    "\n",
    "# Define relationships to keep (水质之间的链接)\n",
    "relationships_keep = [\n",
    "    ('SF', 'Spro'), ('SF', 'Sac'), ('Sac', 'SH2S'), ('SSO4', 'SH2S'), \n",
    "    ('Sh', 'SH2S'), ('XS', 'SF'), ('SH2S', 'SCH4'), ('Sac', 'SCH4'), \n",
    "    ('Sh', 'SCH4'), ('SF', 'Sh')\n",
    "]\n",
    "\n",
    "# Define environmental and water quality features\n",
    "environmental_features = ['ORP', 'V', 'DO', 'pH']\n",
    "water_quality_features = ['SF', 'Spro', 'Sac', 'Sh', 'SSO4', 'SH2S', 'XS', 'SCH4']\n",
    "\n",
    "# Define the number of environmental-to-water-quality links to generate\n",
    "num_env_wq_links = 23  # Same as the number of links to remove\n",
    "\n",
    "# Generate all possible environmental-to-water-quality relationships\n",
    "all_possible_env_wq = [(env, wq) for env in environmental_features for wq in water_quality_features]\n",
    "\n",
    "# Remove any existing environmental-to-water-quality relationships if present\n",
    "# Since in relationships_keep these are only water quality to water quality, no need to remove\n",
    "# But ensure that in relationships_keep there are no environmental-to-water-quality links\n",
    "\n",
    "# Randomly sample 23 unique environmental-to-water-quality links\n",
    "random_env_wq = random.sample(all_possible_env_wq, num_env_wq_links)\n",
    "\n",
    "# Combine the kept relationships and the new environmental-to-water-quality relationships\n",
    "relationships = relationships_keep + random_env_wq\n",
    "\n",
    "# Generate edge_index tensor\n",
    "edge_index = torch.tensor([[node_mapping[src], node_mapping[dst]] for src, dst in relationships], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# ============================\n",
    "# 3. Model Definition\n",
    "# ============================\n",
    "\n",
    "class FlexibleGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate, activation, num_layers, aggregation_type):\n",
    "        super(FlexibleGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        # Aggregation function\n",
    "        if aggregation_type == 'mean':\n",
    "            self.agg_fn = global_mean_pool\n",
    "        elif aggregation_type == 'sum':\n",
    "            self.agg_fn = global_sum_pool\n",
    "        elif aggregation_type == 'max':\n",
    "            self.agg_fn = global_max_pool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation type: {aggregation_type}\")\n",
    "\n",
    "        # Define GAT layers\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            out_dim = hidden_layers[i] if i < len(hidden_layers) else hidden_layers[-1]\n",
    "            heads = 8  # You can make this a hyperparameter if desired\n",
    "            concat = True if i < num_layers - 1 else False  # Don't concatenate in the last layer\n",
    "            self.gat_layers.append(GATConv(prev_dim, out_dim, heads=heads, concat=concat, dropout=dropout_rate))\n",
    "            prev_dim = out_dim * heads if concat else out_dim\n",
    "\n",
    "        # Define a fully connected layer for regression\n",
    "        self.fc = torch.nn.Linear(prev_dim, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = self.activation_fn(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.agg_fn(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================\n",
    "# 4. Dataset Preparation\n",
    "# ============================\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_tensor, edge_index, target_col=-1):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        self.x = data_tensor[:, :-1]\n",
    "        self.y = data_tensor[:, target_col].unsqueeze(1)\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Single graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = Data(x=self.x, edge_index=self.edge_index, y=self.y)\n",
    "        return data\n",
    "\n",
    "dataset = GraphDataset(data_tensor, edge_index)\n",
    "\n",
    "# ============================\n",
    "# 5. Evaluation Function\n",
    "# ============================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            targets.append(data.y.cpu().numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    targets = np.vstack(targets)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, preds)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# ============================\n",
    "# 6. Cross-Validation and Hyperparameter Search\n",
    "# ============================\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_result = None\n",
    "best_val_r2 = -np.inf\n",
    "\n",
    "# Initialize a list to store all results\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting randomized search with five-fold cross-validation...\\n\")\n",
    "for idx, params in enumerate(tqdm(param_list, desc=\"Hyperparameter combinations\")):\n",
    "    fold_metrics = {\n",
    "        'train_mae': [],\n",
    "        'train_mse': [],\n",
    "        'train_rmse': [],\n",
    "        'train_r2': [],\n",
    "        'val_mae': [],\n",
    "        'val_mse': [],\n",
    "        'val_rmse': [],\n",
    "        'val_r2': []\n",
    "    }\n",
    "    \n",
    "    # Convert the entire dataset to a NumPy array for indexing\n",
    "    X = data_tensor[:, :-1].numpy()\n",
    "    y = data_tensor[:, -1].numpy()\n",
    "    \n",
    "    # Perform K-Fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        train_x = torch.tensor(X[train_idx], dtype=torch.float32)\n",
    "        train_y = torch.tensor(y[train_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        val_x = torch.tensor(X[val_idx], dtype=torch.float32)\n",
    "        val_y = torch.tensor(y[val_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Create masks\n",
    "        num_nodes = data_tensor.size(0)\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        \n",
    "        # Create a single Data object with masks\n",
    "        data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1))\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Create DataLoader for the entire graph\n",
    "        loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = FlexibleGNN(\n",
    "            input_dim=X.shape[1],\n",
    "            hidden_layers=params['hidden_layer_sizes'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            activation=params['activation'],\n",
    "            num_layers=params['num_layers'],\n",
    "            aggregation_type=params['aggregation_type']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'rmsprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {params['optimizer']}\")\n",
    "        \n",
    "        # Define learning rate scheduler\n",
    "        if params['learning_rate_scheduler'] == 'constant':\n",
    "            scheduler = None\n",
    "        elif params['learning_rate_scheduler'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "        elif params['learning_rate_scheduler'] == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['num_epochs'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported learning rate scheduler: {params['learning_rate_scheduler']}\")\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Evaluate on training fold\n",
    "        train_preds = out[data.train_mask].detach().cpu().numpy()\n",
    "        train_targets = data.y[data.train_mask].detach().cpu().numpy()\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "        train_mse = mean_squared_error(train_targets, train_preds)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        fold_metrics['train_mae'].append(train_mae)\n",
    "        fold_metrics['train_mse'].append(train_mse)\n",
    "        fold_metrics['train_rmse'].append(train_rmse)\n",
    "        fold_metrics['train_r2'].append(train_r2)\n",
    "        \n",
    "        # Evaluate on validation fold\n",
    "        val_preds = out[data.val_mask].detach().cpu().numpy()\n",
    "        val_targets = data.y[data.val_mask].detach().cpu().numpy()\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_mse = mean_squared_error(val_targets, val_preds)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        fold_metrics['val_mae'].append(val_mae)\n",
    "        fold_metrics['val_mse'].append(val_mse)\n",
    "        fold_metrics['val_rmse'].append(val_rmse)\n",
    "        fold_metrics['val_r2'].append(val_r2)\n",
    "    \n",
    "    # Aggregate metrics across folds\n",
    "    avg_metrics = {metric: np.mean(values) for metric, values in fold_metrics.items()}\n",
    "    avg_metrics['params'] = params\n",
    "    all_results.append(avg_metrics)\n",
    "    \n",
    "    # Update best result based on validation R2\n",
    "    if avg_metrics['val_r2'] > best_val_r2:\n",
    "        best_val_r2 = avg_metrics['val_r2']\n",
    "        best_result = avg_metrics\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis\n",
    "# ============================\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display top 10 hyperparameter combinations based on validation R²\n",
    "top_results = results_df.sort_values(by='val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop hyperparameter combinations based on average validation R²:\")\n",
    "print(top_results[['params', 'val_r2']])\n",
    "\n",
    "# Save all results to Excel for further analysis\n",
    "results_df.to_excel('EcoGNNknowledge_Model_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results saved to 'EcoGNNknowledge_Model_Model_AllResults.xlsx'\")\n",
    "\n",
    "# ============================\n",
    "# 8. Final Model Training (Optional)\n",
    "# ============================\n",
    "\n",
    "# Optionally, retrain the model on the entire dataset using the best hyperparameters\n",
    "# Note: This step is optional and depends on whether you need a final model for deployment\n",
    "\n",
    "# Initialize the final model\n",
    "final_model = FlexibleGNN(\n",
    "    input_dim=data_tensor.size(1) - 1,  # Number of features\n",
    "    hidden_layers=best_result['params']['hidden_layer_sizes'],\n",
    "    dropout_rate=best_result['params']['dropout_rate'],\n",
    "    activation=best_result['params']['activation'],\n",
    "    num_layers=best_result['params']['num_layers'],\n",
    "    aggregation_type=best_result['params']['aggregation_type']\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "if best_result['params']['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_result['params']['learning_rate'], weight_decay=best_result['params']['weight_decay'])\n",
    "elif best_result['params']['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(final_model.parameters(), lr=best_result['params']['learning_rate'], weight_decay=best_result['params']['weight_decay'])\n",
    "elif best_result['params']['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(final_model.parameters(), lr=best_result['params']['learning_rate'], weight_decay=best_result['params']['weight_decay'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {best_result['params']['optimizer']}\")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "if best_result['params']['learning_rate_scheduler'] == 'constant':\n",
    "    scheduler = None\n",
    "elif best_result['params']['learning_rate_scheduler'] == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "elif best_result['params']['learning_rate_scheduler'] == 'cosine':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=best_result['params']['num_epochs'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported learning rate scheduler: {best_result['params']['learning_rate_scheduler']}\")\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Create a single Data object for the entire dataset\n",
    "full_data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1)).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "final_loader = DataLoader([full_data], batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop for the final model\n",
    "final_model.train()\n",
    "for epoch in range(best_result['params']['num_epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "    loss = criterion(out, full_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{best_result['params']['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Final Evaluation\n",
    "# ============================\n",
    "\n",
    "# Evaluate the final model using cross-validation metrics\n",
    "# Since we've already used cross-validation to select hyperparameters, we'll skip re-evaluating\n",
    "\n",
    "# However, if you retrain the model on the entire dataset, you can compute metrics directly\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))\n",
    "    preds = out.cpu().numpy()\n",
    "    targets = full_data.y.cpu().numpy()\n",
    "    final_mae = mean_absolute_error(targets, preds)\n",
    "    final_mse = mean_squared_error(targets, preds)\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    final_r2 = r2_score(targets, preds)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}, MSE: {final_mse:.4f}, RMSE: {final_rmse:.4f}, R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save Best Hyperparameters and Results\n",
    "# ============================\n",
    "\n",
    "# Save the best hyperparameters and performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_result['params']],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save to Excel\n",
    "results_summary.to_excel('EcoGNNknowledge_Model_BestResults.xlsx', index=False)\n",
    "print(\"\\nBest model results saved to 'EcoGNNknowledge_Model_BestResults.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_sum_pool, global_max_pool\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, ParameterSampler\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# 1. Setup and Configuration\n",
    "# ============================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': [(64,), (128,), (64, 64), (128, 128), (64, 128, 64)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_epochs': [50, 100, 200],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'dropout_rate': [0.0, 0.2, 0.5],\n",
    "    'weight_decay': [0.0, 0.0001, 0.001, 0.01],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'aggregation_type': ['mean', 'sum', 'max'],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'learning_rate_scheduler': ['constant', 'step', 'cosine']\n",
    "}\n",
    "\n",
    "# Define the number of random samples from the search space\n",
    "n_iter = 100  # Adjust based on computational resources\n",
    "\n",
    "# Generate random hyperparameter combinations\n",
    "param_list = list(ParameterSampler(search_space, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============================\n",
    "# 2. Data Loading and Processing\n",
    "# ============================\n",
    "\n",
    "# Read the single dataset file\n",
    "data_path = 'path_to_your_data.csv'  # Replace with your actual file path\n",
    "data_df = pd.read_csv(data_path).dropna()\n",
    "\n",
    "# Convert DataFrame to tensor\n",
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32)\n",
    "\n",
    "# Define feature names and mapping (ensure these match your CSV columns)\n",
    "features = ['ORP', 'V', 'DO', 'pH', 'SF', 'Spro', 'Sac', 'Sh', 'SSO4', 'SH2S', 'XS', 'SCH4']\n",
    "node_mapping = {feat: idx for idx, feat in enumerate(features)}\n",
    "\n",
    "# ============================\n",
    "# 2.1. Define and Modify Relationships\n",
    "# ============================\n",
    "\n",
    "# Define external relationships to retain\n",
    "external_relationships = [\n",
    "    ('ORP', 'SF'), ('ORP', 'Spro'), ('ORP', 'Sac'), ('ORP', 'Sh'),\n",
    "    ('V', 'Sac'), ('V', 'Sh'), ('V', 'SSO4'), ('V', 'SH2S'), ('V', 'XS'), ('V', 'SCH4'),\n",
    "    ('DO', 'SF'), ('DO', 'Spro'), ('DO', 'Sac'), ('DO', 'Sh'), ('DO', 'SH2S'), ('DO', 'XS'), ('DO', 'SCH4'),\n",
    "    ('pH', 'SF'), ('pH', 'Spro'), ('pH', 'Sac'), ('pH', 'Sh'), ('pH', 'SSO4'), ('pH', 'XS')\n",
    "]\n",
    "\n",
    "# Define internal relationships to remove\n",
    "internal_relationships_to_remove = [\n",
    "    ('SF', 'Spro'), ('SF', 'Sac'), ('Sac', 'SH2S'), ('SSO4', 'SH2S'), ('Sh', 'SH2S'),\n",
    "    ('XS', 'SF'), ('SH2S', 'SCH4'), ('Sac', 'SCH4'), ('Sh', 'SCH4'), ('SF', 'Sh')\n",
    "]\n",
    "\n",
    "# Define internal features\n",
    "internal_features = ['SF', 'Spro', 'Sac', 'Sh', 'SSO4', 'SH2S', 'XS', 'SCH4']\n",
    "\n",
    "# Generate all possible internal single-direction relationships excluding self-links\n",
    "all_possible_internal_relationships = [\n",
    "    (src, dst) for src in internal_features for dst in internal_features\n",
    "    if src != dst\n",
    "]\n",
    "\n",
    "# Remove the relationships that are to be excluded\n",
    "remaining_internal_relationships = list(set(all_possible_internal_relationships) - set(internal_relationships_to_remove))\n",
    "\n",
    "# Check if there are enough possible relationships to sample\n",
    "if len(remaining_internal_relationships) < 10:\n",
    "    raise ValueError(\"Not enough possible internal relationships to sample from.\")\n",
    "\n",
    "# Randomly sample 10 new internal relationships\n",
    "new_internal_relationships = random.sample(remaining_internal_relationships, 10)\n",
    "\n",
    "# Combine external relationships with new internal relationships\n",
    "relationships = external_relationships + new_internal_relationships\n",
    "\n",
    "print(f\"Total relationships after modification: {len(relationships)}\")\n",
    "print(\"Relationships:\")\n",
    "for rel in relationships:\n",
    "    print(rel)\n",
    "\n",
    "# Generate edge_index tensor\n",
    "edge_index = torch.tensor([[node_mapping[src], node_mapping[dst]] for src, dst in relationships], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# ============================\n",
    "# 3. Model Definition\n",
    "# ============================\n",
    "\n",
    "class FlexibleGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate, activation, num_layers, aggregation_type):\n",
    "        super(FlexibleGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        # Aggregation function\n",
    "        if aggregation_type == 'mean':\n",
    "            self.agg_fn = global_mean_pool\n",
    "        elif aggregation_type == 'sum':\n",
    "            self.agg_fn = global_sum_pool\n",
    "        elif aggregation_type == 'max':\n",
    "            self.agg_fn = global_max_pool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation type: {aggregation_type}\")\n",
    "\n",
    "        # Define GAT layers\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        self.activations = torch.nn.ModuleList()\n",
    "        self.dropouts = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            out_dim = hidden_layers[i] if i < len(hidden_layers) else hidden_layers[-1]\n",
    "            heads = 8  # You can make this a hyperparameter if desired\n",
    "            concat = True if i < num_layers - 1 else False  # Don't concatenate in the last layer\n",
    "            self.gat_layers.append(GATConv(prev_dim, out_dim, heads=heads, concat=concat, dropout=dropout_rate))\n",
    "            prev_dim = out_dim * heads if concat else out_dim\n",
    "\n",
    "        # Define a fully connected layer for regression\n",
    "        self.fc = torch.nn.Linear(prev_dim, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = self.activation_fn(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.agg_fn(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================\n",
    "# 4. Dataset Preparation\n",
    "# ============================\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_tensor, edge_index, target_col=-1):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        self.x = data_tensor[:, :-1]\n",
    "        self.y = data_tensor[:, target_col].unsqueeze(1)\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Single graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = Data(x=self.x, edge_index=self.edge_index, y=self.y)\n",
    "        return data\n",
    "\n",
    "dataset = GraphDataset(data_tensor, edge_index)\n",
    "\n",
    "# ============================\n",
    "# 5. Evaluation Function\n",
    "# ============================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            targets.append(data.y.cpu().numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    targets = np.vstack(targets)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, preds)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# ============================\n",
    "# 6. Cross-Validation and Hyperparameter Search\n",
    "# ============================\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_result = None\n",
    "best_val_r2 = -np.inf\n",
    "\n",
    "# Initialize a list to store all results\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting randomized search with five-fold cross-validation...\\n\")\n",
    "for idx, params in enumerate(tqdm(param_list, desc=\"Hyperparameter combinations\")):\n",
    "    fold_metrics = {\n",
    "        'train_mae': [],\n",
    "        'train_mse': [],\n",
    "        'train_rmse': [],\n",
    "        'train_r2': [],\n",
    "        'val_mae': [],\n",
    "        'val_mse': [],\n",
    "        'val_rmse': [],\n",
    "        'val_r2': []\n",
    "    }\n",
    "    \n",
    "    # Convert the entire dataset to a NumPy array for indexing\n",
    "    X = data_tensor[:, :-1].numpy()\n",
    "    y = data_tensor[:, -1].numpy()\n",
    "    \n",
    "    # Perform K-Fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        train_x = torch.tensor(X[train_idx], dtype=torch.float32)\n",
    "        train_y = torch.tensor(y[train_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        val_x = torch.tensor(X[val_idx], dtype=torch.float32)\n",
    "        val_y = torch.tensor(y[val_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Create masks\n",
    "        num_nodes = data_tensor.size(0)\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        \n",
    "        # Create a single Data object with masks\n",
    "        data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1))\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Create DataLoader for the entire graph\n",
    "        loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = FlexibleGNN(\n",
    "            input_dim=X.shape[1],\n",
    "            hidden_layers=params['hidden_layer_sizes'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            activation=params['activation'],\n",
    "            num_layers=params['num_layers'],\n",
    "            aggregation_type=params['aggregation_type']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'rmsprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {params['optimizer']}\")\n",
    "        \n",
    "        # Define learning rate scheduler\n",
    "        if params['learning_rate_scheduler'] == 'constant':\n",
    "            scheduler = None\n",
    "        elif params['learning_rate_scheduler'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "        elif params['learning_rate_scheduler'] == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['num_epochs'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported learning rate scheduler: {params['learning_rate_scheduler']}\")\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Evaluate on training fold\n",
    "        train_preds = out[data.train_mask].detach().cpu().numpy()\n",
    "        train_targets = data.y[data.train_mask].detach().cpu().numpy()\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "        train_mse = mean_squared_error(train_targets, train_preds)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        fold_metrics['train_mae'].append(train_mae)\n",
    "        fold_metrics['train_mse'].append(train_mse)\n",
    "        fold_metrics['train_rmse'].append(train_rmse)\n",
    "        fold_metrics['train_r2'].append(train_r2)\n",
    "        \n",
    "        # Evaluate on validation fold\n",
    "        val_preds = out[data.val_mask].detach().cpu().numpy()\n",
    "        val_targets = data.y[data.val_mask].detach().cpu().numpy()\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_mse = mean_squared_error(val_targets, val_preds)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        fold_metrics['val_mae'].append(val_mae)\n",
    "        fold_metrics['val_mse'].append(val_mse)\n",
    "        fold_metrics['val_rmse'].append(val_rmse)\n",
    "        fold_metrics['val_r2'].append(val_r2)\n",
    "    \n",
    "    # Aggregate metrics across folds\n",
    "    avg_metrics = {metric: np.mean(values) for metric, values in fold_metrics.items()}\n",
    "    avg_metrics['params'] = params\n",
    "    all_results.append(avg_metrics)\n",
    "    \n",
    "    # Update best result based on validation R2\n",
    "    if avg_metrics['val_r2'] > best_val_r2:\n",
    "        best_val_r2 = avg_metrics['val_r2']\n",
    "        best_result = avg_metrics\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis\n",
    "# ============================\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display top 10 hyperparameter combinations based on validation R²\n",
    "top_results = results_df.sort_values(by='val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop hyperparameter combinations based on average validation R²:\")\n",
    "print(top_results[['params', 'val_r2']])\n",
    "\n",
    "# Save all results to Excel for further analysis\n",
    "results_df.to_excel('EcoGNNSEM_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results saved to 'EcoGNNSEM_Model_AllResults.xlsx'\")\n",
    "\n",
    "# Extract best hyperparameters\n",
    "best_params = best_result['params']\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "\n",
    "# ============================\n",
    "# 8. Final Model Training (Optional)\n",
    "# ============================\n",
    "\n",
    "# Optionally, retrain the model on the entire dataset using the best hyperparameters\n",
    "# Note: This step is optional and depends on whether you need a final model for deployment\n",
    "\n",
    "# Initialize the final model\n",
    "final_model = FlexibleGNN(\n",
    "    input_dim=data_tensor.size(1) - 1,  # Number of features\n",
    "    hidden_layers=best_params['hidden_layer_sizes'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    activation=best_params['activation'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    aggregation_type=best_params['aggregation_type']\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "if best_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {best_params['optimizer']}\")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "if best_params['learning_rate_scheduler'] == 'constant':\n",
    "    scheduler = None\n",
    "elif best_params['learning_rate_scheduler'] == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "elif best_params['learning_rate_scheduler'] == 'cosine':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=best_params['num_epochs'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported learning rate scheduler: {best_params['learning_rate_scheduler']}\")\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Create a single Data object for the entire dataset\n",
    "full_data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1)).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "final_loader = DataLoader([full_data], batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop for the final model\n",
    "final_model.train()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "    loss = criterion(out, full_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{best_params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Final Evaluation\n",
    "# ============================\n",
    "\n",
    "# Evaluate the final model using cross-validation metrics\n",
    "# Since we've already used cross-validation to select hyperparameters, we'll skip re-evaluating\n",
    "\n",
    "# However, if you retrain the model on the entire dataset, you can compute metrics directly\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))\n",
    "    preds = out.cpu().numpy()\n",
    "    targets = full_data.y.cpu().numpy()\n",
    "    final_mae = mean_absolute_error(targets, preds)\n",
    "    final_mse = mean_squared_error(targets, preds)\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    final_r2 = r2_score(targets, preds)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}, MSE: {final_mse:.4f}, RMSE: {final_rmse:.4f}, R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save Best Hyperparameters and Results\n",
    "# ============================\n",
    "\n",
    "# Save the best hyperparameters and performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save to Excel\n",
    "results_summary.to_excel('EcoGNNSEM_Model_BestResults.xlsx', index=False)\n",
    "print(\"\\nBest model results saved to 'EcoGNNSEM_Model_BestResults.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbd5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, global_sum_pool, global_max_pool\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, ParameterSampler\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# 1. Setup and Configuration\n",
    "# ============================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'hidden_layer_sizes': [(64,), (128,), (64, 64), (128, 128), (64, 128, 64)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'num_epochs': [50, 100, 200],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'dropout_rate': [0.0, 0.2, 0.5],\n",
    "    'weight_decay': [0.0, 0.0001, 0.001, 0.01],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'aggregation_type': ['mean', 'sum', 'max'],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'learning_rate_scheduler': ['constant', 'step', 'cosine']\n",
    "}\n",
    "\n",
    "# Define the number of random samples from the search space\n",
    "n_iter = 100  # Adjust based on computational resources\n",
    "\n",
    "# Generate random hyperparameter combinations\n",
    "param_list = list(ParameterSampler(search_space, n_iter=n_iter, random_state=42))\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============================\n",
    "# 2. Data Loading and Processing\n",
    "# ============================\n",
    "\n",
    "# Read the single dataset file\n",
    "data_path = 'path_to_your_data.csv'  # Replace with your actual file path\n",
    "data_df = pd.read_csv(data_path).dropna()\n",
    "\n",
    "# Convert DataFrame to tensor\n",
    "data_tensor = torch.tensor(data_df.values, dtype=torch.float32)\n",
    "\n",
    "# Define feature names and mapping (ensure these match your CSV columns)\n",
    "features = ['ORP', 'V', 'DO', 'pH', 'SF', 'Spro', 'Sac', 'Sh', 'SSO4', 'SH2S', 'XS', 'SCH4']\n",
    "node_mapping = {feat: idx for idx, feat in enumerate(features)}\n",
    "\n",
    "# Define causal relationships (edges)\n",
    "relationships = [\n",
    "    ('ORP', 'SF'), ('ORP', 'Spro'), ('ORP', 'Sac'), ('ORP', 'Sh'),\n",
    "    ('V', 'Sac'), ('V', 'Sh'), ('V', 'SSO4'), ('V', 'SH2S'), ('V', 'XS'), ('V', 'SCH4'),\n",
    "    ('DO', 'SF'), ('DO', 'Spro'), ('DO', 'Sac'), ('DO', 'Sh'), ('DO', 'SH2S'), ('DO', 'XS'), ('DO', 'SCH4'),\n",
    "    ('pH', 'SF'), ('pH', 'Spro'), ('pH', 'Sac'), ('pH', 'Sh'), ('pH', 'SSO4'), ('pH', 'XS'),\n",
    "    ('SF', 'Spro'), ('SF', 'Sac'), ('Sac', 'SH2S'), ('SSO4', 'SH2S'), ('Sh', 'SH2S'),\n",
    "    ('XS', 'SF'), ('SH2S', 'SCH4'), ('Sac', 'SCH4'), ('Sh', 'SCH4'), ('SF', 'Sh')\n",
    "]\n",
    "\n",
    "# Function to generate random edges\n",
    "def generate_random_edges(node_mapping, existing_edges, num_random):\n",
    "    all_nodes = list(node_mapping.values())\n",
    "    existing_edge_set = set(existing_edges)\n",
    "    random_edges = set()\n",
    "    \n",
    "    while len(random_edges) < num_random:\n",
    "        src = random.choice(all_nodes)\n",
    "        dst = random.choice(all_nodes)\n",
    "        if src == dst:\n",
    "            continue  # Skip self-loops\n",
    "        edge = (src, dst)\n",
    "        if edge in existing_edge_set or edge in random_edges:\n",
    "            continue  # Skip existing edges and duplicates\n",
    "        random_edges.add(edge)\n",
    "    \n",
    "    return list(random_edges)\n",
    "\n",
    "# Convert feature-based relationships to index-based edges\n",
    "existing_edges = [(node_mapping[src], node_mapping[dst]) for src, dst in relationships]\n",
    "\n",
    "# Number of random edges to generate\n",
    "num_random_edges = len(existing_edges)\n",
    "\n",
    "# Generate random edges\n",
    "random_edges = generate_random_edges(node_mapping, existing_edges, num_random_edges)\n",
    "\n",
    "# Combine existing edges with random edges\n",
    "all_edges = existing_edges + random_edges\n",
    "\n",
    "# Convert edge list to tensor\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# ============================\n",
    "# 3. Model Definition\n",
    "# ============================\n",
    "\n",
    "class FlexibleGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate, activation, num_layers, aggregation_type):\n",
    "        super(FlexibleGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation_fn = torch.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = torch.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "        # Aggregation function\n",
    "        if aggregation_type == 'mean':\n",
    "            self.agg_fn = global_mean_pool\n",
    "        elif aggregation_type == 'sum':\n",
    "            self.agg_fn = global_sum_pool\n",
    "        elif aggregation_type == 'max':\n",
    "            self.agg_fn = global_max_pool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation type: {aggregation_type}\")\n",
    "\n",
    "        # Define GAT layers\n",
    "        self.gat_layers = torch.nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for i in range(num_layers):\n",
    "            out_dim = hidden_layers[i] if i < len(hidden_layers) else hidden_layers[-1]\n",
    "            heads = 8  # You can make this a hyperparameter if desired\n",
    "            concat = True if i < num_layers - 1 else False  # Don't concatenate in the last layer\n",
    "            self.gat_layers.append(GATConv(prev_dim, out_dim, heads=heads, concat=concat, dropout=dropout_rate))\n",
    "            prev_dim = out_dim * heads if concat else out_dim\n",
    "\n",
    "        # Define a fully connected layer for regression\n",
    "        self.fc = torch.nn.Linear(prev_dim, 1)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for gat in self.gat_layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = self.activation_fn(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.agg_fn(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================\n",
    "# 4. Dataset Preparation\n",
    "# ============================\n",
    "\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_tensor, edge_index, target_col=-1):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        self.x = data_tensor[:, :-1]\n",
    "        self.y = data_tensor[:, target_col].unsqueeze(1)\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Single graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = Data(x=self.x, edge_index=self.edge_index, y=self.y)\n",
    "        return data\n",
    "\n",
    "dataset = GraphDataset(data_tensor, edge_index)\n",
    "\n",
    "# ============================\n",
    "# 5. Evaluation Function\n",
    "# ============================\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            targets.append(data.y.cpu().numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    targets = np.vstack(targets)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(targets, preds)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# ============================\n",
    "# 6. Cross-Validation and Hyperparameter Search\n",
    "# ============================\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_result = None\n",
    "best_val_r2 = -np.inf\n",
    "\n",
    "# Initialize a list to store all results\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting randomized search with five-fold cross-validation...\\n\")\n",
    "for idx, params in enumerate(tqdm(param_list, desc=\"Hyperparameter combinations\")):\n",
    "    fold_metrics = {\n",
    "        'train_mae': [],\n",
    "        'train_mse': [],\n",
    "        'train_rmse': [],\n",
    "        'train_r2': [],\n",
    "        'val_mae': [],\n",
    "        'val_mse': [],\n",
    "        'val_rmse': [],\n",
    "        'val_r2': []\n",
    "    }\n",
    "    \n",
    "    # Convert the entire dataset to a NumPy array for indexing\n",
    "    X = data_tensor[:, :-1].numpy()\n",
    "    y = data_tensor[:, -1].numpy()\n",
    "    \n",
    "    # Perform K-Fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        train_x = torch.tensor(X[train_idx], dtype=torch.float32)\n",
    "        train_y = torch.tensor(y[train_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        val_x = torch.tensor(X[val_idx], dtype=torch.float32)\n",
    "        val_y = torch.tensor(y[val_idx], dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Create masks\n",
    "        num_nodes = data_tensor.size(0)\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        \n",
    "        # Create a single Data object with masks\n",
    "        data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1))\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Create DataLoader for the entire graph\n",
    "        loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = FlexibleGNN(\n",
    "            input_dim=X.shape[1],\n",
    "            hidden_layers=params['hidden_layer_sizes'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            activation=params['activation'],\n",
    "            num_layers=params['num_layers'],\n",
    "            aggregation_type=params['aggregation_type']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Define optimizer\n",
    "        if params['optimizer'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        elif params['optimizer'] == 'rmsprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {params['optimizer']}\")\n",
    "        \n",
    "        # Define learning rate scheduler\n",
    "        if params['learning_rate_scheduler'] == 'constant':\n",
    "            scheduler = None\n",
    "        elif params['learning_rate_scheduler'] == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "        elif params['learning_rate_scheduler'] == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['num_epochs'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported learning rate scheduler: {params['learning_rate_scheduler']}\")\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(params['num_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Evaluate on training fold\n",
    "        train_preds = out[data.train_mask].detach().cpu().numpy()\n",
    "        train_targets = data.y[data.train_mask].detach().cpu().numpy()\n",
    "        train_mae = mean_absolute_error(train_targets, train_preds)\n",
    "        train_mse = mean_squared_error(train_targets, train_preds)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        fold_metrics['train_mae'].append(train_mae)\n",
    "        fold_metrics['train_mse'].append(train_mse)\n",
    "        fold_metrics['train_rmse'].append(train_rmse)\n",
    "        fold_metrics['train_r2'].append(train_r2)\n",
    "        \n",
    "        # Evaluate on validation fold\n",
    "        val_preds = out[data.val_mask].detach().cpu().numpy()\n",
    "        val_targets = data.y[data.val_mask].detach().cpu().numpy()\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_mse = mean_squared_error(val_targets, val_preds)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        fold_metrics['val_mae'].append(val_mae)\n",
    "        fold_metrics['val_mse'].append(val_mse)\n",
    "        fold_metrics['val_rmse'].append(val_rmse)\n",
    "        fold_metrics['val_r2'].append(val_r2)\n",
    "    \n",
    "    # Aggregate metrics across folds\n",
    "    avg_metrics = {metric: np.mean(values) for metric, values in fold_metrics.items()}\n",
    "    avg_metrics['params'] = params\n",
    "    all_results.append(avg_metrics)\n",
    "    \n",
    "    # Update best result based on validation R2\n",
    "    if avg_metrics['val_r2'] > best_val_r2:\n",
    "        best_val_r2 = avg_metrics['val_r2']\n",
    "        best_result = avg_metrics\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis\n",
    "# ============================\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display top 10 hyperparameter combinations based on validation R²\n",
    "top_results = results_df.sort_values(by='val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop hyperparameter combinations based on average validation R²:\")\n",
    "print(top_results[['params', 'val_r2']])\n",
    "\n",
    "# Save all results to Excel for further analysis\n",
    "results_df.to_excel('EcoGNNRandom_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results saved to 'EcoGNNRandom_Model_AllResults.xlsx'\")\n",
    "\n",
    "# Save top 10 results\n",
    "top_results.to_excel('EcoGNNRandom_Model_Top10Results.xlsx', index=False)\n",
    "print(\"Top 10 hyperparameter combinations have been saved to 'EcoGNNRandom_Model_Top10Results.xlsx'\")\n",
    "\n",
    "# Extract best hyperparameters\n",
    "best_params = best_result['params']\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "\n",
    "# ============================\n",
    "# 8. Final Model Training (Optional)\n",
    "# ============================\n",
    "\n",
    "# Optionally, retrain the model on the entire dataset using the best hyperparameters\n",
    "# Note: This step is optional and depends on whether you need a final model for deployment\n",
    "\n",
    "# Initialize the final model\n",
    "final_model = FlexibleGNN(\n",
    "    input_dim=data_tensor.size(1) - 1,  # Number of features\n",
    "    hidden_layers=best_params['hidden_layer_sizes'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    activation=best_params['activation'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    aggregation_type=best_params['aggregation_type']\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "if best_params['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "elif best_params['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {best_params['optimizer']}\")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "if best_params['learning_rate_scheduler'] == 'constant':\n",
    "    scheduler = None\n",
    "elif best_params['learning_rate_scheduler'] == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "elif best_params['learning_rate_scheduler'] == 'cosine':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=best_params['num_epochs'])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported learning rate scheduler: {best_params['learning_rate_scheduler']}\")\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Create a single Data object for the entire dataset\n",
    "full_data = Data(x=data_tensor[:, :-1], edge_index=edge_index, y=data_tensor[:, -1].unsqueeze(1)).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "final_loader = DataLoader([full_data], batch_size=1, shuffle=True)\n",
    "\n",
    "# Training loop for the final model\n",
    "final_model.train()\n",
    "for epoch in range(best_params['num_epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))  # Batch can be zeros since it's a single graph\n",
    "    loss = criterion(out, full_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{best_params['num_epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Final Evaluation\n",
    "# ============================\n",
    "\n",
    "# Evaluate the final model using cross-validation metrics\n",
    "# Since we've already used cross-validation to select hyperparameters, we'll skip re-evaluating\n",
    "\n",
    "# However, if you retrain the model on the entire dataset, you can compute metrics directly\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = final_model(full_data.x, full_data.edge_index, torch.zeros(full_data.x.size(0), dtype=torch.long).to(device))\n",
    "    preds = out.cpu().numpy()\n",
    "    targets = full_data.y.cpu().numpy()\n",
    "    final_mae = mean_absolute_error(targets, preds)\n",
    "    final_mse = mean_squared_error(targets, preds)\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "    final_r2 = r2_score(targets, preds)\n",
    "\n",
    "print(\"\\nFinal Model Performance on Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}, MSE: {final_mse:.4f}, RMSE: {final_rmse:.4f}, R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save Best Hyperparameters and Results\n",
    "# ============================\n",
    "\n",
    "# Save the best hyperparameters and performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save to Excel\n",
    "results_summary.to_excel('EcoGNNRandom_Model_BestResults.xlsx', index=False)\n",
    "print(\"\\nBest model results saved to 'EcoGNNRandom_Model_BestResults.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a53aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "# Define the SewerX function representing the mechanistic model\n",
    "def SewerX(t, y, kHAC, kSRBAC, kMPBAC):\n",
    "    # Unpack state variables\n",
    "    SF, Spro, Sac, Sh, SCH4, SSO4, SH2S, Se, XS, XI, Xaci, Xace, XMAac, XMAh, XSRBpr, XSRBac, XSRBh = y\n",
    "    \n",
    "    # Define constants\n",
    "    khydro = 3\n",
    "    kaci = 6\n",
    "    kace = 6\n",
    "    KF = 10\n",
    "    # Use optimization parameters\n",
    "    kMAac = kHAC\n",
    "    KMAac = 210\n",
    "    kMAh = kSRBAC\n",
    "    KMAh = 0.1\n",
    "    kSRBpro = kMPBAC\n",
    "    KSRBpro = 110\n",
    "    kSRBac = 7.1\n",
    "    KSRBac = 220\n",
    "    kSRBh = 26.7\n",
    "    KSRBh = 0.1\n",
    "    KSO4 = 1.8\n",
    "    kh2s = 1\n",
    "    kdecaci = 0.02\n",
    "    kdecace = 0.02\n",
    "    kdecMAac = 0.015\n",
    "    kdecMAh = 0.01\n",
    "    kdecSRBpro = 0.010\n",
    "    kdecSRBac = 0.015\n",
    "    kdecSRBh = 0.01\n",
    "    f1 = 0.78\n",
    "    f2 = 0.22\n",
    "    f3 = 0.67\n",
    "    f4 = 0.33\n",
    "    n = 1.65\n",
    "    Yaci = 0.1\n",
    "    Yace = 0.06\n",
    "    YMAac = 0.0317\n",
    "    YMAh = 0.0403\n",
    "    YSRBac = 0.0329\n",
    "    YSRBpro = 0.0342\n",
    "    YSRBh = 0.0366\n",
    "\n",
    "    # Define the A matrix based on the model equations\n",
    "    A = np.array([\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [-1, f1*(1-Yaci), f2*(1-Yaci), 0, 0, 0, 0, 0, 0, 0, Yaci, 0, 0, 0, 0, 0, 0],\n",
    "        [-1, 0, f3*(1-Yace), f4*(1-Yace), 0, 0, 0, 0, 0, 0, 0, Yace, 0, 0, 0, 0, 0],\n",
    "        [0, 0, -1, 0, 1-YMAac, 0, 0, 0, 0, 0, 0, 0, YMAac, 0, 0, 0, 0],\n",
    "        [0, 0, 0, -1, 1-YMAh, 0, 0, 0, 0, 0, 0, 0, 0, YMAh, 0, 0, 0],\n",
    "        [0, -1, 0, 0, 0, (YSRBpro-1)/2, (1-YSRBpro)/2, 0, 0, 0, 0, 0, 0, 0, YSRBpro, 0, 0],\n",
    "        [0, 0, -1, 0, 0, (YSRBac-1)/2, (1-YSRBac)/2, 0, 0, 0, 0, 0, 0, 0, 0, YSRBac, 0],\n",
    "        [0, 0, 0, -1, 0, (YSRBh-1)/2, (1-YSRBh)/2, 0, 0, 0, 0, 0, 0, 0, 0, 0, YSRBh],\n",
    "        [0, 0, 0, 0, 0, 0, -1, -n, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, -1, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, -1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, -1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, -1, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0, -1, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0, 0, -1, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0, 0, 0, 0, 0, -1]\n",
    "    ]).transpose()\n",
    "\n",
    "    # Define the B vector based on the model equations\n",
    "    B = [\n",
    "        khydro * XS,\n",
    "        kaci * SF / (KF + SF) * Xaci,\n",
    "        kace * SF / (KF + SF) * Xace,\n",
    "        kMAac * Sac / (KMAac + Sac) * XMAac,\n",
    "        kMAh * Sh / (KMAh + Sh) * XMAh,\n",
    "        kSRBpro * Spro * SSO4 * XSRBpr / (KSRBpro + Spro) / (KSO4 + KSO4),\n",
    "        kSRBac * Sac * SSO4 * XSRBac / (KSRBac + Sac) / (KSO4 + SSO4),\n",
    "        kSRBh * Sh * SSO4 * XSRBh / (KSRBh + Sh) / (KSO4 + SSO4),\n",
    "        kh2s * SH2S * Se,\n",
    "        kdecaci * Xaci,\n",
    "        kdecace * Xace,\n",
    "        kdecMAac * XMAac,\n",
    "        kdecMAh * XMAh,\n",
    "        kdecSRBpro * XSRBpr,\n",
    "        kdecSRBac * XSRBac,\n",
    "        kdecSRBh * XSRBh      \n",
    "    ]\n",
    "\n",
    "    # Calculate the derivatives based on the A matrix and B vector\n",
    "    return [np.dot(A[i, :], B) for i in range(17)]\n",
    "\n",
    "# Load the datasets\n",
    "influent_data = pd.read_csv(\"influent.csv\")\n",
    "position1_data = pd.read_csv(\"position1.csv\")\n",
    "position2_data = pd.read_csv(\"position2.csv\")\n",
    "position3_data = pd.read_csv(\"position3.csv\")\n",
    "position4_data = pd.read_csv(\"position4.csv\")\n",
    "\n",
    "# Ensure all datasets have the same number of samples\n",
    "num_samples = len(influent_data)\n",
    "assert len(position1_data) == num_samples, \"Position1 data rows do not match influent data.\"\n",
    "assert len(position2_data) == num_samples, \"Position2 data rows do not match influent data.\"\n",
    "assert len(position3_data) == num_samples, \"Position3 data rows do not match influent data.\"\n",
    "assert len(position4_data) == num_samples, \"Position4 data rows do not match influent data.\"\n",
    "\n",
    "# Define the reaction times corresponding to each position\n",
    "reaction_times = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "# Define the fitness function to evaluate the model's performance\n",
    "def evaluate_model(individual):\n",
    "    \"\"\"\n",
    "    Evaluate the model by calculating the total Mean Squared Error (MSE)\n",
    "    between the model predictions and the observed data across all positions and samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - individual: A list containing the parameters [kHAC, kSRBAC, kMPBAC] to be optimized.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing the total MSE.\n",
    "    \"\"\"\n",
    "    kHAC, kSRBAC, kMPBAC = individual\n",
    "    total_mse = 0.0\n",
    "\n",
    "    for index in range(num_samples):\n",
    "        # Get the initial conditions for the current sample from the influent data\n",
    "        y0 = influent_data.iloc[index].values\n",
    "\n",
    "        # Define the time span and evaluation points for the ODE solver\n",
    "        t_span = (0, 2.0)\n",
    "        t_eval = reaction_times\n",
    "\n",
    "        # Solve the ODEs using the SewerX function with the current parameters\n",
    "        try:\n",
    "            sol = solve_ivp(\n",
    "                SewerX, \n",
    "                t_span, \n",
    "                y0, \n",
    "                args=(kHAC, kSRBAC, kMPBAC), \n",
    "                t_eval=t_eval, \n",
    "                method='LSODA'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Assign a large error if the solver fails\n",
    "            return (1e6,)\n",
    "\n",
    "        if not sol.success:\n",
    "            # Assign a large error if the solver did not succeed\n",
    "            return (1e6,)\n",
    "\n",
    "        # Transpose the solution to have each row correspond to a time point\n",
    "        y = sol.y.T\n",
    "\n",
    "        # Compare the model output with observed data for each reaction time\n",
    "        for i, t in enumerate(reaction_times):\n",
    "            # Get the model output at the current time point\n",
    "            model_output = y[i, :]\n",
    "\n",
    "            # Retrieve the corresponding observed data based on the reaction time\n",
    "            if i == 0:\n",
    "                observed = position1_data.iloc[index].values\n",
    "            elif i == 1:\n",
    "                observed = position2_data.iloc[index].values\n",
    "            elif i == 2:\n",
    "                observed = position3_data.iloc[index].values\n",
    "            elif i == 3:\n",
    "                observed = position4_data.iloc[index].values\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Calculate the Mean Squared Error (MSE) between model output and observed data\n",
    "            mse = np.mean((model_output - observed) ** 2)\n",
    "            total_mse += mse\n",
    "\n",
    "    return (total_mse,)\n",
    "\n",
    "# Setup the Genetic Algorithm using DEAP\n",
    "# Define the fitness as minimizing the total MSE\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "# Define an individual as a list with fitness attribute\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "# Initialize the toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Define the parameter ranges for kHAC, kSRBAC, and kMPBAC\n",
    "kHAC_min, kHAC_max = 0.1, 20.0\n",
    "kSRBAC_min, kSRBAC_max = 0.1, 20.0\n",
    "kMPBAC_min, kMPBAC_max = 0.1, 20.0\n",
    "\n",
    "# Register the attribute generators for each parameter\n",
    "toolbox.register(\"attr_kHAC\", random.uniform, kHAC_min, kHAC_max)\n",
    "toolbox.register(\"attr_kSRBAC\", random.uniform, kSRBAC_min, kSRBAC_max)\n",
    "toolbox.register(\"attr_kMPBAC\", random.uniform, kMPBAC_min, kMPBAC_max)\n",
    "\n",
    "# Register the individual and population generators\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, \n",
    "                 (toolbox.attr_kHAC, toolbox.attr_kSRBAC, toolbox.attr_kMPBAC), n=1)\n",
    "toolbox.register(\"population\", tools.initPopulation, list, toolbox.individual)\n",
    "\n",
    "# Register the evaluation function\n",
    "toolbox.register(\"evaluate\", evaluate_model)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1.0, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the Genetic Algorithm for parameter optimization.\n",
    "    Implements an early stopping mechanism based on lack of improvement.\n",
    "    \"\"\"\n",
    "    # GA parameters\n",
    "    population_size = 50\n",
    "    max_generations = 100  # Set a high maximum number of generations\n",
    "    crossover_prob = 0.7    # Probability of mating\n",
    "    mutation_prob = 0.2     # Probability of mutation\n",
    "    patience = 10           # Number of generations to wait for improvement\n",
    "    improvement_threshold = 1e-6  # Minimum improvement to reset patience\n",
    "\n",
    "    # Initialize the population\n",
    "    pop = toolbox.population(n=population_size)\n",
    "\n",
    "    # Set up multiprocessing pool for parallel evaluation\n",
    "    pool = multiprocessing.Pool()\n",
    "    toolbox.register(\"map\", pool.map)\n",
    "\n",
    "    # Initialize tracking variables for early stopping\n",
    "    best_fitness = None\n",
    "    no_improve_count = 0\n",
    "    best_individual = None\n",
    "\n",
    "    # Evolutionary loop\n",
    "    for gen in range(max_generations):\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Apply crossover on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < crossover_prob:\n",
    "                toolbox.mate(child1, child2)\n",
    "                # Invalidate fitness values after crossover\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        # Apply mutation on the offspring\n",
    "        for mutant in offspring:\n",
    "            if random.random() < mutation_prob:\n",
    "                toolbox.mutate(mutant)\n",
    "                # Invalidate fitness values after mutation\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Replace the old population with the new offspring\n",
    "        pop[:] = offspring\n",
    "\n",
    "        # Gather all the fitnesses in the population\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "        min_fit = min(fits)\n",
    "        avg_fit = np.mean(fits)\n",
    "        max_fit = max(fits)\n",
    "\n",
    "        # Print the statistics for the current generation\n",
    "        print(f\"Generation {gen+1}: Min Fitness = {min_fit}, Avg Fitness = {avg_fit}, Max Fitness = {max_fit}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if best_fitness is None or best_fitness - min_fit > improvement_threshold:\n",
    "            best_fitness = min_fit\n",
    "            no_improve_count = 0\n",
    "            # Update the best individual found so far\n",
    "            best_individual = tools.selBest(pop, 1)[0]\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            print(f\"No significant improvement. Patience count: {no_improve_count}/{patience}\")\n",
    "\n",
    "        # Check if early stopping condition is met\n",
    "        if no_improve_count >= patience:\n",
    "            print(f\"No improvement in the last {patience} generations. Stopping early.\")\n",
    "            break\n",
    "\n",
    "    # Close the multiprocessing pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Select and print the best individual\n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    print(f\"Best Individual: {best_ind}\")\n",
    "    print(f\"Best Fitness (Total MSE): {best_ind.fitness.values[0]}\")\n",
    "\n",
    "    # Save the best results to a text file\n",
    "    with open(\"C:/Users/Van/Desktop/ga_results.txt\", \"w\") as f:\n",
    "        f.write(f\"Best Individual: {best_ind}\\n\")\n",
    "        f.write(f\"Best Fitness (Total MSE): {best_ind.fitness.values[0]}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN2",
   "language": "python",
   "name": "gnn2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
