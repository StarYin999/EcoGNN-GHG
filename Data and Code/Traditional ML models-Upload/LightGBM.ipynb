{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1030a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  # For saving models\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Import LightGBM's LGBMRegressor\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except ImportError:\n",
    "    print(\"LightGBM is not installed. Installing now...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"lightgbm\"])\n",
    "    from lightgbm import LGBMRegressor\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed for Reproducibility\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for NumPy and Python's random module to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can choose any seed value you prefer\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Define the path to your CSV data file\n",
    "data_path = 'path_to_your_data.csv'  # <-- Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "# Extract features and target variable as NumPy arrays\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values in features and target\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"\\nMissing values detected. Performing imputation...\")\n",
    "    # Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "    print(\"Missing values imputed with column means.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Pipeline and Hyperparameter Grid\n",
    "# ============================\n",
    "\n",
    "# Create a pipeline that first scales the data and then applies LGBMRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lgbmregressor', LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'lgbmregressor__num_leaves': [31, 64, 128, 256],          # Number of leaves in full trees\n",
    "    'lgbmregressor__max_depth': [10, 20, 30, 40],            # Maximum depth of the tree\n",
    "    'lgbmregressor__n_estimators': [100, 200, 300],          # Number of boosting iterations\n",
    "    'lgbmregressor__learning_rate': [0.01, 0.1, 0.5],        # Boosting learning rate\n",
    "    'lgbmregressor__subsample': [0.6, 0.8, 1.0],             # Fraction of samples to be used for fitting the individual base learners\n",
    "    'lgbmregressor__colsample_bytree': ['auto', 'sqrt', 'log2'],  # Fraction of features to be used for fitting the individual base learners\n",
    "    'lgbmregressor__reg_alpha': [0.0, 0.1, 0.5, 1.0],        # L1 regularization term on weights\n",
    "    'lgbmregressor__reg_lambda': [0.0, 0.1, 0.5, 1.0]        # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Grid Search\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and hyperparameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',          # Using R² as the evaluation metric\n",
    "    cv=kf,\n",
    "    n_jobs=-1,             # Utilize all available CPU cores\n",
    "    verbose=2              # Verbosity level: 0, 1, or 2\n",
    ")\n",
    "\n",
    "print(\"Starting five-fold cross-validation and grid search...\\n\")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Extract all grid search results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for clarity\n",
    "selected_columns = [\n",
    "    'param_lgbmregressor__num_leaves',\n",
    "    'param_lgbmregressor__max_depth',\n",
    "    'param_lgbmregressor__n_estimators',\n",
    "    'param_lgbmregressor__learning_rate',\n",
    "    'param_lgbmregressor__subsample',\n",
    "    'param_lgbmregressor__colsample_bytree',\n",
    "    'param_lgbmregressor__reg_alpha',\n",
    "    'param_lgbmregressor__reg_lambda',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'std_train_score'\n",
    "]\n",
    "results_selected = results_df[selected_columns]\n",
    "\n",
    "# Rename columns for better readability\n",
    "results_selected = results_selected.rename(columns={\n",
    "    'param_lgbmregressor__num_leaves': 'num_leaves',\n",
    "    'param_lgbmregressor__max_depth': 'max_depth',\n",
    "    'param_lgbmregressor__n_estimators': 'n_estimators',\n",
    "    'param_lgbmregressor__learning_rate': 'learning_rate',\n",
    "    'param_lgbmregressor__subsample': 'subsample',\n",
    "    'param_lgbmregressor__colsample_bytree': 'colsample_bytree',\n",
    "    'param_lgbmregressor__reg_alpha': 'reg_alpha',\n",
    "    'param_lgbmregressor__reg_lambda': 'reg_lambda',\n",
    "    'mean_test_score': 'mean_val_r2',\n",
    "    'std_test_score': 'std_val_r2',\n",
    "    'mean_train_score': 'mean_train_r2',\n",
    "    'std_train_score': 'std_train_r2'\n",
    "})\n",
    "\n",
    "# Sort the results by mean validation R² in descending order and select top 10\n",
    "top_results = results_selected.sort_values(by='mean_val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results)\n",
    "\n",
    "# Save all grid search results to an Excel file for further analysis\n",
    "results_selected.to_excel('LGBM_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'LGBM_Model_AllResults.xlsx'.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best validation R² score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Validation R² Score: {best_score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model and Scaler\n",
    "# ============================\n",
    "\n",
    "# Save the pipeline (which includes both scaler and model) using joblib\n",
    "model_filename = 'LGBM_FinalModel_Pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"\\nFinal model pipeline (including scaler) has been saved as '{model_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Create a dictionary with the best hyperparameters and final performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_filename = 'LGBM_Model_BestResults.xlsx'\n",
    "results_summary.to_excel(summary_filename, index=False)\n",
    "print(f\"\\nBest model results have been saved to '{summary_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 12. Conclusion\n",
    "# ============================\n",
    "\n",
    "print(\"\\nScript execution completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN2",
   "language": "python",
   "name": "gnn2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
