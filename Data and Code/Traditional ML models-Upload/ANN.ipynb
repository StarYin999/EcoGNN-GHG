{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Import Necessary Libraries\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib  # For saving models\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# ============================\n",
    "# 2. Set Random Seed for Reproducibility\n",
    "# ============================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for NumPy and Python's random module to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  # You can choose any seed value you prefer\n",
    "\n",
    "# ============================\n",
    "# 3. Load and Inspect Data\n",
    "# ============================\n",
    "\n",
    "# Define the path to your CSV data file\n",
    "data_path = 'path_to_your_data.csv'  # <-- Replace with your actual file path\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    print(\"Dataset loaded successfully.\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the specified path: {data_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(data_df.info())\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "print(\"\\nFirst 5 Rows of the Dataset:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# ============================\n",
    "# 4. Data Preprocessing\n",
    "# ============================\n",
    "\n",
    "# Assume the last column is the target variable (adjust if necessary)\n",
    "feature_cols = data_df.columns[:-1]\n",
    "target_col = data_df.columns[-1]\n",
    "\n",
    "# Extract features and target variable as NumPy arrays\n",
    "X = data_df[feature_cols].values\n",
    "y = data_df[target_col].values\n",
    "\n",
    "# Check for missing values in features and target\n",
    "if np.isnan(X).any() or np.isnan(y).any():\n",
    "    print(\"\\nMissing values detected. Performing imputation...\")\n",
    "    # Replace NaNs with the mean of each column\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))\n",
    "    y = np.nan_to_num(y, nan=np.nanmean(y))\n",
    "    print(\"Missing values imputed with column means.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# ============================\n",
    "# 5. Define Pipeline and Hyperparameter Grid\n",
    "# ============================\n",
    "\n",
    "# Create a pipeline that first scales the data and then applies MLPRegressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlpregressor', MLPRegressor(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "# Since some hyperparameters are only applicable to certain solvers,\n",
    "# we define separate parameter grids for each solver\n",
    "param_grid = [\n",
    "    {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['lbfgs'],\n",
    "        'mlpregressor__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'mlpregressor__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'mlpregressor__max_iter': [200, 300, 500],\n",
    "        'mlpregressor__batch_size': [32, 64, 128, 256],\n",
    "        'mlpregressor__early_stopping': [True, False],\n",
    "        'mlpregressor__validation_fraction': [0.1, 0.2, 0.3]\n",
    "        # Note: 'momentum', 'beta_1', 'beta_2', 'epsilon' are not applicable to 'lbfgs'\n",
    "    },\n",
    "    {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['sgd'],\n",
    "        'mlpregressor__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'mlpregressor__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'mlpregressor__max_iter': [200, 300, 500],\n",
    "        'mlpregressor__batch_size': [32, 64, 128, 256],\n",
    "        'mlpregressor__momentum': [0.0, 0.5, 0.9],\n",
    "        'mlpregressor__early_stopping': [True, False],\n",
    "        'mlpregressor__validation_fraction': [0.1, 0.2, 0.3]\n",
    "        # 'beta_1', 'beta_2', 'epsilon' are not applicable to 'sgd'\n",
    "    },\n",
    "    {\n",
    "        'mlpregressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50)],\n",
    "        'mlpregressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlpregressor__solver': ['adam'],\n",
    "        'mlpregressor__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'mlpregressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'mlpregressor__learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'mlpregressor__max_iter': [200, 300, 500],\n",
    "        'mlpregressor__batch_size': [32, 64, 128, 256],\n",
    "        'mlpregressor__beta_1': [0.9, 0.95, 0.99],\n",
    "        'mlpregressor__beta_2': [0.999, 0.995, 0.99],\n",
    "        'mlpregressor__epsilon': [1e-8, 1e-7, 1e-6],\n",
    "        'mlpregressor__early_stopping': [True, False],\n",
    "        'mlpregressor__validation_fraction': [0.1, 0.2, 0.3]\n",
    "        # 'momentum' is not applicable to 'adam'\n",
    "    }\n",
    "]\n",
    "\n",
    "# ============================\n",
    "# 6. Five-Fold Cross-Validation and Grid Search\n",
    "# ============================\n",
    "\n",
    "# Initialize K-Fold cross-validation with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with the pipeline and hyperparameter grid\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',          # Using R² as the evaluation metric\n",
    "    cv=kf,\n",
    "    n_jobs=-1,             # Utilize all available CPU cores\n",
    "    verbose=2              # Verbosity level: 0, 1, or 2\n",
    ")\n",
    "\n",
    "print(\"Starting five-fold cross-validation and grid search...\\n\")\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "# ============================\n",
    "# 7. Results Analysis and Saving\n",
    "# ============================\n",
    "\n",
    "# Extract all grid search results into a DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for clarity\n",
    "selected_columns = [\n",
    "    'param_mlpregressor__hidden_layer_sizes',\n",
    "    'param_mlpregressor__activation',\n",
    "    'param_mlpregressor__solver',\n",
    "    'param_mlpregressor__alpha',\n",
    "    'param_mlpregressor__learning_rate',\n",
    "    'param_mlpregressor__learning_rate_init',\n",
    "    'param_mlpregressor__max_iter',\n",
    "    'param_mlpregressor__batch_size',\n",
    "    'param_mlpregressor__momentum',\n",
    "    'param_mlpregressor__beta_1',\n",
    "    'param_mlpregressor__beta_2',\n",
    "    'param_mlpregressor__epsilon',\n",
    "    'param_mlpregressor__early_stopping',\n",
    "    'param_mlpregressor__validation_fraction',\n",
    "    'mean_test_score',\n",
    "    'std_test_score',\n",
    "    'mean_train_score',\n",
    "    'std_train_score'\n",
    "]\n",
    "results_selected = results_df[selected_columns]\n",
    "\n",
    "# Rename columns for better readability\n",
    "results_selected = results_selected.rename(columns={\n",
    "    'param_mlpregressor__hidden_layer_sizes': 'hidden_layer_sizes',\n",
    "    'param_mlpregressor__activation': 'activation',\n",
    "    'param_mlpregressor__solver': 'solver',\n",
    "    'param_mlpregressor__alpha': 'alpha',\n",
    "    'param_mlpregressor__learning_rate': 'learning_rate',\n",
    "    'param_mlpregressor__learning_rate_init': 'learning_rate_init',\n",
    "    'param_mlpregressor__max_iter': 'max_iter',\n",
    "    'param_mlpregressor__batch_size': 'batch_size',\n",
    "    'param_mlpregressor__momentum': 'momentum',\n",
    "    'param_mlpregressor__beta_1': 'beta_1',\n",
    "    'param_mlpregressor__beta_2': 'beta_2',\n",
    "    'param_mlpregressor__epsilon': 'epsilon',\n",
    "    'param_mlpregressor__early_stopping': 'early_stopping',\n",
    "    'param_mlpregressor__validation_fraction': 'validation_fraction',\n",
    "    'mean_test_score': 'mean_val_r2',\n",
    "    'std_test_score': 'std_val_r2',\n",
    "    'mean_train_score': 'mean_train_r2',\n",
    "    'std_train_score': 'std_train_r2'\n",
    "})\n",
    "\n",
    "# Sort the results by mean validation R² in descending order and select top 10\n",
    "top_results = results_selected.sort_values(by='mean_val_r2', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Hyperparameter Combinations Based on Average Validation R²:\")\n",
    "print(top_results)\n",
    "\n",
    "# Save all grid search results to an Excel file for further analysis\n",
    "results_selected.to_excel('ANN_Model_AllResults.xlsx', index=False)\n",
    "print(\"\\nAll hyperparameter search results have been saved to 'ANN_Model_AllResults.xlsx'.\")\n",
    "\n",
    "# ============================\n",
    "# 8. Extract Best Hyperparameter Combination\n",
    "# ============================\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest Hyperparameter Combination:\")\n",
    "print(best_params)\n",
    "\n",
    "# Get the best validation R² score\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best Validation R² Score: {best_score:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 9. Train Final Model with Best Hyperparameters\n",
    "# ============================\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "final_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Train the final model on the entire dataset\n",
    "final_pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset\n",
    "y_pred = final_pipeline.predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "final_mae = mean_absolute_error(y, y_pred)\n",
    "final_mse = mean_squared_error(y, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Performance on the Entire Dataset:\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"MSE: {final_mse:.4f}\")\n",
    "print(f\"RMSE: {final_rmse:.4f}\")\n",
    "print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ============================\n",
    "# 10. Save the Best Model and Scaler\n",
    "# ============================\n",
    "\n",
    "# Save the pipeline (which includes both scaler and model) using joblib\n",
    "model_filename = 'ANN_FinalModel_Pipeline.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"\\nFinal model pipeline (including scaler) has been saved as '{model_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 11. Save Best Hyperparameters and Final Results\n",
    "# ============================\n",
    "\n",
    "# Create a dictionary with the best hyperparameters and final performance metrics\n",
    "results_to_save = {\n",
    "    'Best Params': [best_params],\n",
    "    'Final MAE': [final_mae],\n",
    "    'Final MSE': [final_mse],\n",
    "    'Final RMSE': [final_rmse],\n",
    "    'Final R²': [final_r2]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "results_summary = pd.DataFrame(results_to_save)\n",
    "\n",
    "# Save the summary to an Excel file\n",
    "summary_filename = 'ANN_Model_BestResults.xlsx'\n",
    "results_summary.to_excel(summary_filename, index=False)\n",
    "print(f\"\\nBest model results have been saved to '{summary_filename}'.\")\n",
    "\n",
    "# ============================\n",
    "# 12. Conclusion\n",
    "# ============================\n",
    "\n",
    "print(\"\\nScript execution completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN2",
   "language": "python",
   "name": "gnn2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
